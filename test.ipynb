{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcaab75f",
   "metadata": {},
   "source": [
    "### Import Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37233d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv\n",
    "from mlagents_envs.envs.custom_side_channel import CustomDataChannel, StringSideChannel\n",
    "from uuid import UUID\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "import torch.optim as optim\n",
    "\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "from collections import deque\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a186704",
   "metadata": {},
   "source": [
    "### Curriculum Learning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf2bb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CurriculumLearning:\n",
    "    def __init__(self):\n",
    "    \n",
    "        self.train_player = \"LEFT\"\n",
    "        self.course = \"A\"\n",
    "\n",
    "        self.frozen_left_agent = None\n",
    "        self.frozen_right_agent = None\n",
    "        \n",
    "        self.play_times = 0\n",
    "        \n",
    "        self.course_score = 0\n",
    "        self.course_grade = {\n",
    "            \"A\":21,\n",
    "            \"B\":21,\n",
    "            \"C\":21,\n",
    "            \"D\":19\n",
    "        }\n",
    "    \n",
    "    def reset_score(self):\n",
    "        self.course_score = 0\n",
    "    \n",
    "    def courseA_agent(self):\n",
    "        return [0,0,0]\n",
    "    \n",
    "    def courseB_agent(self):\n",
    "        return [np.random.randint(0,3),np.random.randint(0,3),np.random.randint(0,3)]\n",
    "    \n",
    "    def courseC_agent(self):\n",
    "        if self.train_player == \"LEFT\":\n",
    "            return [0,2,0]\n",
    "        \n",
    "        if self.train_player == \"RIGHT\":\n",
    "            return [0,1,0]\n",
    "    \n",
    "    def courseD_agent(self,obs):\n",
    "        if self.train_player == \"LEFT\":\n",
    "            with torch.no_grad():\n",
    "                actions, _, _ = self.frozen_right_agent.select_action(obs)\n",
    "                action = [actions[0],actions[1],actions[2]]\n",
    "            return action\n",
    "        \n",
    "        if self.train_player == \"RIGHT\":\n",
    "            with torch.no_grad():\n",
    "                actions, _, _ = self.frozen_left_agent.select_action(obs)\n",
    "                action = [actions[0],actions[1],actions[2]]\n",
    "            return action\n",
    " \n",
    "    def current_course(self,obs=None):  \n",
    "        if self.course == \"A\":\n",
    "            action = self.courseA_agent()\n",
    "        elif self.course == \"B\":\n",
    "            action = self.courseB_agent()\n",
    "        elif self.course == \"C\":\n",
    "            action = self.courseC_agent()\n",
    "        elif self.course == \"D\":\n",
    "            action = self.courseD_agent(obs)\n",
    "        return action\n",
    "        \n",
    "    def update_score(self,reward,done):\n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                self.course_score += 1\n",
    "    \n",
    "    def course_grad(self):\n",
    "        threshold = self.course_grade[self.course]\n",
    "        \n",
    "        if self.course_score >= threshold:\n",
    "            print(f\"The Course {self.course} passed by {self.train_player} player.\")\n",
    "            print(f\"The grade of this course is {self.course_score}/21 \")\n",
    "            if self.train_player == \"LEFT\":\n",
    "                self.train_player = \"RIGHT\"\n",
    "                \n",
    "            elif self.train_player == \"RIGHT\":\n",
    "                self.train_player = \"LEFT\"\n",
    "            \n",
    "        \n",
    "        elif self.course_score <= threshold:\n",
    "            print(f\"The {self.train_player} player not to pass this course.\")\n",
    "            print(f\"The grade of this course is {self.course_score}/21\")\n",
    "        self.reset_score()\n",
    "\n",
    "    \n",
    "    def switch_course(self,player1_PPO,player2_PPO):\n",
    "        if self.course == \"A\":\n",
    "            self.course = \"B\"\n",
    "            \n",
    "        elif self.course == \"B\":\n",
    "            self.course = \"C\"\n",
    "        \n",
    "        elif self.course == \"C\":\n",
    "            from copy import deepcopy\n",
    "            self.frozen_left_agent = deepcopy(player1_PPO)\n",
    "            self.frozen_right_agent = deepcopy(player2_PPO)\n",
    "            self.course = \"D\"\n",
    "        \n",
    "        print(f\"Curriculum Learning switch to next level {self.course}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaeb97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameProcessor:\n",
    "    def __init__(self,stack_frame = 4):\n",
    "        self.stack_frame = stack_frame\n",
    "        self.stack = deque(maxlen = stack_frame)\n",
    "        \n",
    "    def concat(self):\n",
    "        return np.concatenate(list(self.stack),axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ed3675",
   "metadata": {},
   "source": [
    "### Actor Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870d0a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels , channels, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.silu = nn.SiLU()\n",
    "    \n",
    "        self.conv2 = nn.Conv2d(channels, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.silu = nn.SiLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x1 = self.silu(self.bn1(self.conv1(x)))\n",
    "        x2 = self.silu(self.bn2(self.conv2(x1)))\n",
    "        return x1 + x2\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self,action_head = [3,3,3]): # input -> (60,138,4)\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4,16,kernel_size = 4, stride = 2, padding = 1), # (30,69,16)\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(16,32,kerel_size = 4, stride = 2, padding = 1), #  (15,35,32)\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(32,64,kerel_size = 6, stride = 1, padding = 0), # (10,30,64)\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.SiLU(),\n",
    "            ResidualBlock(64),\n",
    "            ResidualBlock(64),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.silu = nn.SiLU()\n",
    "        \n",
    "        self.fc1 = nn.Linear(19200,512)\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.fc2 = nn.Linear(512,256)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.action_head1 = nn.Linear(256,action_head[0])\n",
    "        self.action_head2 = nn.Linear(256,action_head[1])\n",
    "        self.action_head3 = nn.Linear(256,action_head[2])\n",
    "        \n",
    "        self.value_head = nn.Linear(256,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.conv(x.float() / 255.0)\n",
    "        x = self.silu(self.bn1(self.fc1(x)))\n",
    "        shared_feat = self.silu(self.bn2(self.fc2(x)))\n",
    "        \n",
    "        policy = (\n",
    "            self.action_head1(shared_feat),\n",
    "            self.action_head2(shared_feat),\n",
    "            self.action_head3(shared_feat),\n",
    "        )\n",
    "        \n",
    "        value = self.value_head(shared_feat)\n",
    "        \n",
    "        return policy, value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4fd3a7",
   "metadata": {},
   "source": [
    "### PPO Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0cb8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.obs = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        \n",
    "        self.actions = []\n",
    "        self.logprobs = []\n",
    "        self.values = []\n",
    "    \n",
    "    def clear(self):\n",
    "        self .__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d449781b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(\n",
    "        self, \n",
    "        net:ActorCritic,\n",
    "        optimizer,\n",
    "        device,\n",
    "        \n",
    "        gamma = 0.99,\n",
    "        lambd = 0.95,\n",
    "        clip_eps = 0.2,\n",
    "        entropy_coef = 0.01,\n",
    "        value_loss_coef = 0.5):\n",
    "        \n",
    "        self.net = net\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.lambd = lambd\n",
    "        self.clip_eps = clip_eps\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_loss_coef = value_loss_coef\n",
    "        \n",
    "    def select_action(self,stack_obs):\n",
    "        stack_obs = torch.tensor(stack_obs, dtype = torch.float32, device = self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits, value = self.net(stack_obs) # ([a,b,c],[a,b,c],[a,b,c]), value\n",
    "        \n",
    "        actions = []\n",
    "        logprobs = []\n",
    "        for logit in logits:\n",
    "            dist = Categorical(logits = logit)\n",
    "            a = dist.sample()\n",
    "            actions.append(a.item())\n",
    "            logprobs.append(dist.log_prob(a))\n",
    "            \n",
    "        total_logprob = torch.sum(torch.stack(logprobs))\n",
    "        return np.array(actions), total_logprob.item(), value.item()\n",
    "    \n",
    "    def compute_gae(self, buffer: RolloutBuffer, last_value = 0):\n",
    "        rewards = buffer.rewards\n",
    "        values  = buffer.values + [last_value]\n",
    "        dones   = buffer.dones\n",
    "        \n",
    "        advantages = []\n",
    "        gae = []\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            delta = rewards[t] + self.gamma * values[t+1] * (1 - dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.lambd * (1 - dones[t]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "            \n",
    "        returns = [adv + val for adv, val in zip(advantages, values[:-1])]\n",
    "        return advantages, returns\n",
    "    \n",
    "\n",
    "    def update(self, buffer: RolloutBuffer):\n",
    "        advantages, returns = self.compute_gae(buffer)\n",
    "        \n",
    "        obs = torch.tensor(buffer.obs, dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(buffer.actions, dtype=torch.int64, device=self.device)\n",
    "        old_logprobs = torch.tensor(buffer.logprobs, dtype=torch.float32, device=self.device)\n",
    "        returns = torch.tensor(returns, dtype=torch.float32, device=self.device)\n",
    "        advantages = torch.tensor(advantages, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        logits, values = self.net(obs)\n",
    "        logp = sum([torch.distributions.Categorical(logits=l).log_prob(actions[:,i]) \n",
    "                    for i,l in enumerate(logits)])\n",
    "        \n",
    "        ratio = torch.exp(logp - old_logprobs)\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1-self.clip_eps, 1+self.clip_eps) * advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "        entropy = sum([torch.distributions.Categorical(logits=l).entropy().mean() for l in logits])\n",
    "\n",
    "        loss = policy_loss + self.value_loss_coef*value_loss - self.entropy_coef * entropy\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        buffer.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8e3c0526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(obs):\n",
    "    gray = 0.299 * obs[0] + 0.587 * obs[1] + 0.114 * obs[2]\n",
    "    return gray[...,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3ec38",
   "metadata": {},
   "source": [
    "### Main Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0dcd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.015\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
    "update_threshold = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6110dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 84, 168)\n",
      "Training interrupted\n"
     ]
    }
   ],
   "source": [
    "string_channel = StringSideChannel()\n",
    "channel = CustomDataChannel()\n",
    "\n",
    "reward_cum = [0,0]\n",
    "\n",
    "channel.send_data(serve=212, p1=reward_cum[0], p2=reward_cum[1])\n",
    "\n",
    "unity_env = UnityEnvironment(r\"C:/Users/junmi\\Documents\\dPickleball BuildFiless (1)\\dPickleball BuildFiles/Training\\Windows\\dp.exe\",side_channels=[string_channel, channel])\n",
    "env = UnityParallelEnv(unity_env)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "current_reward = 0 \n",
    "previous_reward = 0\n",
    "episode_reward = [0,0]\n",
    "\n",
    "update_freq = 0\n",
    "\n",
    "curriculum_learning = CurriculumLearning()\n",
    "\n",
    "action_head = [3,3,3]\n",
    "player1_net = ActorCritic(action_head).to(device)\n",
    "player2_net = ActorCritic(action_head).to(device)\n",
    " \n",
    "optim_player1 = optim.AdamW(player1_net.parameters(), lr = learning_rate, eps = 1e-5)\n",
    "optim_player2 = optim.AdamW(player2_net.parameters(), lr = learning_rate, eps = 1e-5)\n",
    " \n",
    "player1_PPO = PPO(player1_net,optim_player1,device)\n",
    "player2_PPO = PPO(player2_net,optim_player2,device) \n",
    " \n",
    "first_frame = env.reset()['PAgent1?team=0?agent_id=0'][\"observation\"][0]\n",
    "first_frame = preprocess(first_frame)\n",
    "frame_processor = FrameProcessor(4)\n",
    "\n",
    "for i in range(frame_processor.stack_frame):\n",
    "    frame_processor.stack.append(first_frame)\n",
    "    frame_stack = frame_processor.concat()\n",
    "\n",
    "step = 0\n",
    "player1_buffer = RolloutBuffer()\n",
    "player2_buffer = RolloutBuffer()\n",
    "\n",
    "try:\n",
    "    while env.agents:\n",
    "        \n",
    "        if curriculum_learning.train_player == \"LEFT\":\n",
    "            actions_left, logprob, values = player1_PPO.select_action(frame_stack)\n",
    "            action_right = curriculum_learning.current_course(frame_stack)\n",
    "        \n",
    "        if curriculum_learning.train_player == \"RIGHT\":\n",
    "            actions_right, logprob, values = player2_PPO.select_action(frame_stack)\n",
    "            action_left = curriculum_learning.current_course()\n",
    "            \n",
    "        action_left = [actions_left[0],actions_left[1],actions_left[2]]\n",
    "        action_right = [actions_right[0],actions_right[1],actions_right[2]]\n",
    "        \n",
    "        actions = {env.agents[0]:action_left,env.agents[1]:action_right}\n",
    "        \n",
    "        observation, reward, done, info = env.step(actions)\n",
    "        \n",
    "        obs = observation[env.agents[0]]['observation'][0]  # (3,84,168)\n",
    "        obs = obs[:,20:80,15:153] # (3,60,138)\n",
    "        obs = preprocess(obs)\n",
    "        \n",
    "        dones = 0\n",
    "        reward_cum[0] += reward[env.agents[0]]\n",
    "        reward_cum[1] += reward[env.agents[1]]\n",
    "        \n",
    "        current_reward = reward_cum[0]\n",
    "        \n",
    "        if current_reward != previous_reward:\n",
    "            update_freq += 1\n",
    "            dones = 1\n",
    "        \n",
    "        previous_reward = current_reward\n",
    "        \n",
    "        if curriculum_learning.train_player == \"LEFT\":\n",
    "            player1_buffer.obs.append(obs)\n",
    "            player1_buffer.rewards.append(reward[env.agents[0]])\n",
    "            player1_buffer.dones.append(dones)\n",
    "            player1_buffer.actions.append(action_left)\n",
    "            player1_buffer.logprobs.append(logprob)\n",
    "            player1_buffer.values.append(values)\n",
    "        \n",
    "        if curriculum_learning.train_player == \"RIGHT\":\n",
    "            player2_buffer.obs.append(obs)\n",
    "            player2_buffer.rewards.append(reward[env.agents[1]])\n",
    "            player2_buffer.dones.append(dones)\n",
    "            player2_buffer.actions.append(action_right)\n",
    "            player2_buffer.logprobs.append(logprob)\n",
    "            player2_buffer.values.append(values)\n",
    "        \n",
    "        if step % 4 == 0:\n",
    "            frame_processor.stack.append(obs)\n",
    "        frame_stack = frame_processor.concat()\n",
    "        \n",
    "        if done[env.agents[0]] or done[env.agents[1]]:\n",
    "            sys.exit()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        if update_freq == update_threshold:\n",
    "            if curriculum_learning.train_player == \"LEFT\":\n",
    "                last_value = player1_PPO.net(torch.tensor(frame_stack, dtype=torch.float32, device=device).unsqueeze(0))[1].item()\n",
    "                player1_PPO.update(player1_buffer, last_value)\n",
    "            else:\n",
    "                last_value = player2_PPO.net(torch.tensor(frame_stack, dtype=torch.float32, device=device).unsqueeze(0))[1].item()\n",
    "                player2_PPO.update(player2_buffer, last_value)\n",
    "            update_freq = 0\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted\")\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e42db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpickleball",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
