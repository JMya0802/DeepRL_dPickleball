{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3899ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃█</td></tr><tr><td>train/done_loss</td><td>█▁▁</td></tr><tr><td>train/kl_loss</td><td>▁▁▁</td></tr><tr><td>train/kl_loss_raw</td><td>▁▇█</td></tr><tr><td>train/recon_loss</td><td>█▁▁</td></tr><tr><td>train/reward_loss</td><td>█▁▂</td></tr><tr><td>train/total_loss</td><td>█▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>train/done_loss</td><td>0.0</td></tr><tr><td>train/kl_loss</td><td>2</td></tr><tr><td>train/kl_loss_raw</td><td>1.08824</td></tr><tr><td>train/recon_loss</td><td>0.00646</td></tr><tr><td>train/reward_loss</td><td>0.00501</td></tr><tr><td>train/total_loss</td><td>0.21147</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wm_train_rssm_v1</strong> at: <a href='https://wandb.ai/hongttisme-xiamen-university-malaysia/WorldModel-Training-RSSM/runs/uoxcbvnj' target=\"_blank\">https://wandb.ai/hongttisme-xiamen-university-malaysia/WorldModel-Training-RSSM/runs/uoxcbvnj</a><br> View project at: <a href='https://wandb.ai/hongttisme-xiamen-university-malaysia/WorldModel-Training-RSSM' target=\"_blank\">https://wandb.ai/hongttisme-xiamen-university-malaysia/WorldModel-Training-RSSM</a><br>Synced 5 W&B file(s), 4 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251019_161310-uoxcbvnj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\tan04\\Documents\\CodePlace\\AI\\pickleBall\\DeepReinforcementLearning_SelfPlay_dPickleball\\wandb\\run-20251019_161550-46qhq3zf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hongttisme-xiamen-university-malaysia/WorldModel-Training-RSSM/runs/46qhq3zf' target=\"_blank\">wm_train_rssm_v1</a></strong> to <a href='https://wandb.ai/hongttisme-xiamen-university-malaysia/WorldModel-Training-RSSM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hongttisme-xiamen-university-malaysia/WorldModel-Training-RSSM' target=\"_blank\">https://wandb.ai/hongttisme-xiamen-university-malaysia/WorldModel-Training-RSSM</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hongttisme-xiamen-university-malaysia/WorldModel-Training-RSSM/runs/46qhq3zf' target=\"_blank\">https://wandb.ai/hongttisme-xiamen-university-malaysia/WorldModel-Training-RSSM/runs/46qhq3zf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Found 464 episode files.\n",
      "Loading data into memory...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 464/464 [00:09<00:00, 48.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sequence (chunk) indices...\n",
      "Total of 905264 training chunks available.\n",
      "Encoder CNN output shape: torch.Size([64, 5, 15]), Flattened: 4800\n",
      "Model parameters: 5942723\n",
      "Starting training from scratch.\n",
      "\n",
      "--- Epoch 1 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 14145/14145 [1:26:01<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 finished. Total Loss: 0.2058, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 2 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 14145/14145 [1:25:59<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 finished. Total Loss: 0.2053, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 3 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 14145/14145 [1:25:56<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 finished. Total Loss: 0.2045, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 4 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 14145/14145 [1:25:44<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 finished. Total Loss: 0.2048, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 5 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 14145/14145 [1:25:32<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 finished. Total Loss: 0.2057, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 6 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 14145/14145 [1:25:35<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 finished. Total Loss: 0.2044, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 7 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 14145/14145 [1:25:36<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 finished. Total Loss: 0.2049, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 8 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 14145/14145 [1:25:36<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 finished. Total Loss: 0.2044, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 9 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 14145/14145 [1:25:35<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 finished. Total Loss: 0.2043, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 10 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 14145/14145 [1:25:34<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 finished. Total Loss: 0.2047, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 11 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 14145/14145 [1:25:34<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 finished. Total Loss: 0.2046, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 12 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 14145/14145 [1:25:34<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 finished. Total Loss: 0.2043, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 13 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 14145/14145 [1:25:42<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 finished. Total Loss: 0.2046, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 14 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 14145/14145 [1:25:43<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 finished. Total Loss: 0.2047, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 15 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 14145/14145 [1:25:43<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 finished. Total Loss: 0.2051, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 16 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 14145/14145 [1:25:45<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 finished. Total Loss: 0.2043, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 17 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 14145/14145 [1:25:45<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 finished. Total Loss: 0.2046, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 18 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 14145/14145 [1:25:45<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 finished. Total Loss: 0.2047, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 19 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 14145/14145 [1:25:43<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 finished. Total Loss: 0.2051, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 20 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 14145/14145 [1:25:33<00:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 finished. Total Loss: 0.2043, KL Loss: 2.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 21 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 14145/14145 [1:25:32<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 finished. Total Loss: 0.2047, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 22 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 14145/14145 [1:25:32<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 finished. Total Loss: 0.2042, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 23 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 14145/14145 [1:25:32<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 finished. Total Loss: 0.2051, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 24 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 14145/14145 [1:25:31<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 finished. Total Loss: 0.2043, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 25 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 14145/14145 [1:25:29<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 finished. Total Loss: 0.2047, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 26 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 14145/14145 [1:25:30<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 finished. Total Loss: 0.2047, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 27 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27: 100%|██████████| 14145/14145 [1:25:28<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 finished. Total Loss: 0.2046, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 28 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28: 100%|██████████| 14145/14145 [1:25:28<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 finished. Total Loss: 0.2046, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 29 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29: 100%|██████████| 14145/14145 [1:25:30<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 finished. Total Loss: 0.2047, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 30 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30: 100%|██████████| 14145/14145 [1:25:31<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 finished. Total Loss: 0.2046, KL Loss: 2.0000\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to: saved_world_model_rssm\\latest_checkpoint.pth\n",
      "\n",
      "--- Epoch 31 / 50 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31:  96%|█████████▌| 13587/14145 [1:22:11<03:22,  2.76it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 451\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    449\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    452\u001b[0m     s_t, a_t, r_next, d_next, s_next \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m    454\u001b[0m     s_t \u001b[38;5;241m=\u001b[39m s_t\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\tan04\\anaconda3\\envs\\dpickleball\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tan04\\anaconda3\\envs\\dpickleball\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    740\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\tan04\\anaconda3\\envs\\dpickleball\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\tan04\\anaconda3\\envs\\dpickleball\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\tan04\\anaconda3\\envs\\dpickleball\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[4], line 135\u001b[0m, in \u001b[0;36mWorldModelDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    133\u001b[0m     branch_actions_clamped \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(branch_actions, \u001b[38;5;241m0\u001b[39m, branch_dim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    134\u001b[0m     branch_one_hot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39meye(branch_dim)[branch_actions_clamped]\n\u001b[1;32m--> 135\u001b[0m     a_t_one_hot[:, i\u001b[38;5;241m*\u001b[39mbranch_dim : (i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mbranch_dim] \u001b[38;5;241m=\u001b[39m branch_one_hot\n\u001b[0;32m    137\u001b[0m s_t_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(s_t_chunk\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m    138\u001b[0m a_t_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(a_t_one_hot\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import typing as tt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.distributions import Normal, kl\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "# --- 1. Configuration (Upgraded) ---\n",
    "\n",
    "config = {\n",
    "    # --- Paths ---\n",
    "    \"DATA_PATH\": \"world_model_data\",\n",
    "    \"CHECKPOINT_DIR\": \"saved_world_model_rssm\", # Changed to directory\n",
    "    # Set this path to resume training, e.g., \"saved_world_model_rssm/latest_checkpoint.pth\"\n",
    "    \"LOAD_CHECKPOINT_PATH\": None, \n",
    "    \"RUN_NAME\": \"wm_train_rssm_v1\",\n",
    "\n",
    "    # --- Training Parameters ---\n",
    "    \"BATCH_SIZE\": 64,\n",
    "    \"CHUNK_LENGTH\": 50,\n",
    "    \"LEARNING_RATE\": 4e-4,\n",
    "    \"NUM_EPOCHS\": 50,\n",
    "    \"DEVICE\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"GRAD_CLIP_NORM\": 100.0,\n",
    "    \"NUM_WORKERS\": 0,\n",
    "\n",
    "    # --- Model Architecture ---\n",
    "    \"INPUT_SHAPE\": (1, 54, 132),          # Observation shape (C, H, W)\n",
    "    \"ACTION_DIM_PER_BRANCH\": 3,\n",
    "    \"NUM_ACTION_BRANCHES\": 3,\n",
    "    \"EMBED_DIM\": 256,\n",
    "    \"RNN_HIDDEN_DIM\": 512,\n",
    "    \"LATENT_DIM\": 512,                  # Stochastic latent dim (z)\n",
    "\n",
    "    # --- Loss Weights (Upgraded) ---\n",
    "    \"RECON_LOSS_WEIGHT\": 1.0,\n",
    "    \"REWARD_LOSS_WEIGHT\": 1.0,\n",
    "    \"DONE_LOSS_WEIGHT\": 1.0,\n",
    "    \"KL_LOSS_WEIGHT\": 0.1,              # Weight for the new KL loss\n",
    "    \"KL_FREE_NATS\": 2.0                 # KL \"free bits\" to prevent posterior collapse\n",
    "}\n",
    "\n",
    "# --- 2. Dataset (Unchanged) ---\n",
    "# ... [你的 WorldModelDataset 代码保持不变] ...\n",
    "class WorldModelDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for loading .npz trajectory data and sampling sequences (chunks).\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir: str, chunk_length: int):\n",
    "        self.chunk_length = chunk_length\n",
    "        self.data_files = glob.glob(os.path.join(data_dir, \"*.npz\"))\n",
    "        print(f\"Found {len(self.data_files)} episode files.\")\n",
    "\n",
    "        self.trajectories = []\n",
    "        self.chunk_indices = []\n",
    "\n",
    "        self.load_data()\n",
    "        self.generate_indices()\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Loads all .npz files into memory.\"\"\"\n",
    "        print(\"Loading data into memory...\")\n",
    "        for file_path in tqdm(self.data_files):\n",
    "            try:\n",
    "                with np.load(file_path) as data:\n",
    "                    obs = data['observations']\n",
    "                    act_p0 = data['actions_p0']\n",
    "                    rew_p0 = data['rewards_p0']\n",
    "                    dones = data['dones']\n",
    "\n",
    "                    s_t = obs[:-1]\n",
    "                    s_next = obs[1:]\n",
    "\n",
    "                    T = len(act_p0)\n",
    "                    if not (len(s_t) == T and len(rew_p0) == T and len(dones) == T and len(s_next) == T):\n",
    "                        print(f\"Warning: Skipping {file_path} due to mismatched data lengths.\")\n",
    "                        continue\n",
    "                    \n",
    "                    if T == 0:\n",
    "                        continue\n",
    "\n",
    "                    self.trajectories.append({\n",
    "                        's_t': s_t,\n",
    "                        'a_t': act_p0,\n",
    "                        'r_next': rew_p0,\n",
    "                        'd_next': dones,\n",
    "                        's_next': s_next\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to load {file_path}: {e}\")\n",
    "\n",
    "    def generate_indices(self):\n",
    "        \"\"\"Creates indices for all possible sequences of `chunk_length`.\"\"\"\n",
    "        print(\"Generating sequence (chunk) indices...\")\n",
    "        for ep_idx, traj in enumerate(self.trajectories):\n",
    "            ep_len = len(traj['a_t'])\n",
    "            if ep_len < self.chunk_length:\n",
    "                continue\n",
    "\n",
    "            for start_idx in range(ep_len - self.chunk_length + 1):\n",
    "                self.chunk_indices.append((ep_idx, start_idx))\n",
    "        print(f\"Total of {len(self.chunk_indices)} training chunks available.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.chunk_indices)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> tt.Tuple[torch.Tensor, ...]:\n",
    "        ep_idx, start_idx = self.chunk_indices[idx]\n",
    "        end_idx = start_idx + self.chunk_length\n",
    "\n",
    "        traj = self.trajectories[ep_idx]\n",
    "\n",
    "        s_t_chunk = traj['s_t'][start_idx:end_idx]\n",
    "        a_t_chunk = traj['a_t'][start_idx:end_idx]\n",
    "        r_next_chunk = traj['r_next'][start_idx:end_idx]\n",
    "        d_next_chunk = traj['d_next'][start_idx:end_idx]\n",
    "        s_next_chunk = traj['s_next'][start_idx:end_idx]\n",
    "\n",
    "        num_branches = config[\"NUM_ACTION_BRANCHES\"]\n",
    "        branch_dim = config[\"ACTION_DIM_PER_BRANCH\"]\n",
    "        total_action_dim = num_branches * branch_dim\n",
    "\n",
    "        a_t_one_hot = np.zeros((self.chunk_length, total_action_dim), dtype=np.float32)\n",
    "        for i in range(num_branches):\n",
    "            branch_actions = a_t_chunk[:, i]\n",
    "            # Clamp actions to be valid indices\n",
    "            branch_actions_clamped = np.clip(branch_actions, 0, branch_dim - 1).astype(int)\n",
    "            branch_one_hot = np.eye(branch_dim)[branch_actions_clamped]\n",
    "            a_t_one_hot[:, i*branch_dim : (i+1)*branch_dim] = branch_one_hot\n",
    "\n",
    "        s_t_tensor = torch.from_numpy(s_t_chunk.astype(np.float32))\n",
    "        a_t_tensor = torch.from_numpy(a_t_one_hot.astype(np.float32))\n",
    "        r_next_tensor = torch.from_numpy(r_next_chunk.astype(np.float32))\n",
    "        d_next_tensor = torch.from_numpy(d_next_chunk.astype(np.float32))\n",
    "        s_next_tensor = torch.from_numpy(s_next_chunk.astype(np.float32))\n",
    "\n",
    "        return s_t_tensor, a_t_tensor, r_next_tensor, d_next_tensor, s_next_tensor\n",
    "\n",
    "# --- 3. Model Definition (Upgraded to RSSM-like) ---\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes observations (B*T, C, H, W) into embeddings (B*T, embed_dim).\n",
    "    (Unchanged, your architecture is correct for the input size)\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, input_shape: tt.Tuple[int, int, int]):\n",
    "        super().__init__()\n",
    "        c, h, w = input_shape\n",
    "\n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, c, h, w)\n",
    "            dummy_output = self.conv_net(dummy_input)\n",
    "            self.conv_out_shape = dummy_output.shape[1:]\n",
    "            self.cnn_out_dim = dummy_output.numel()\n",
    "            print(f\"Encoder CNN output shape: {self.conv_out_shape}, Flattened: {self.cnn_out_dim}\")\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(self.cnn_out_dim, embed_dim)\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        # Normalize observations\n",
    "        embed = self.conv_net(obs / 255.0) \n",
    "        embed = self.flatten(embed)\n",
    "        embed = self.fc(embed)\n",
    "        return embed\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decodes latent states (B*T, latent_dim) back into observations (B*T, C, H, W).\n",
    "    (Unchanged, your architecture is correct)\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int, cnn_out_dim: int, conv_out_shape: tt.Tuple[int, int, int]):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(latent_dim, cnn_out_dim)\n",
    "        self.unflatten = nn.Unflatten(1, conv_out_shape)\n",
    "\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            # output_padding=(2, 0) is the key to matching the (54, 132) shape\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=8, stride=4, output_padding=(2, 0)),\n",
    "            nn.Sigmoid() # Output pixels between 0 and 1\n",
    "        )\n",
    "\n",
    "    def forward(self, latent: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.fc(latent)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "class WorldModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Stochastic World Model (RSSM-like).\n",
    "    \n",
    "    This model predicts the *next* state (t+1) given the current state (t)\n",
    "    and action (t). It uses the *actual* next observation (s_{t+1}) to\n",
    "    form a posterior distribution, which is trained to match a prior\n",
    "    distribution predicted from history (s_{0...t}, a_t).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_shape: tt.Tuple[int, int, int],\n",
    "                 embed_dim: int,\n",
    "                 action_dim: int,\n",
    "                 rnn_hidden_dim: int,\n",
    "                 latent_dim: int):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(embed_dim, input_shape)\n",
    "        \n",
    "        # RNN to process history (s_0...s_t)\n",
    "        self.rnn = nn.GRU(embed_dim, rnn_hidden_dim, batch_first=True)\n",
    "\n",
    "        # Input to transition models: (h_t, a_t)\n",
    "        transition_input_dim = rnn_hidden_dim + action_dim\n",
    "\n",
    "        # --- Prior Network (The \"Imagination\") ---\n",
    "        # p(z_{t+1} | h_t, a_t)\n",
    "        self.prior_net = nn.Sequential(\n",
    "            nn.Linear(transition_input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2 * latent_dim) # mean and std\n",
    "        )\n",
    "\n",
    "        # --- Posterior Network (The \"Reality-Check\") ---\n",
    "        # q(z_{t+1} | h_t, a_t, s_{t+1})\n",
    "        # We add the embedding of the *next* state\n",
    "        posterior_input_dim = transition_input_dim + embed_dim \n",
    "        self.posterior_net = nn.Sequential(\n",
    "            nn.Linear(posterior_input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2 * latent_dim) # mean and std\n",
    "        )\n",
    "        \n",
    "        # --- Prediction Heads ---\n",
    "        # These predict outcomes from the stochastic latent state z\n",
    "        self.decoder = Decoder(\n",
    "            latent_dim,\n",
    "            self.encoder.cnn_out_dim,\n",
    "            self.encoder.conv_out_shape\n",
    "        )\n",
    "        self.reward_head = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        self.done_head = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def forward(self, \n",
    "                    s_t: torch.Tensor, \n",
    "                    a_t: torch.Tensor, \n",
    "                    s_next: torch.Tensor\n",
    "                ) -> tt.Tuple[torch.Tensor, ...]:\n",
    "            \n",
    "            B, T, C, H, W = s_t.shape\n",
    "\n",
    "            # 1. Encode current observation s_t\n",
    "            # 使用 .reshape() 替代 .view()\n",
    "            s_t_flat = s_t.reshape(B * T, C, H, W) \n",
    "            embed_t_flat = self.encoder(s_t_flat)\n",
    "            # 使用 .reshape() 替代 .view()\n",
    "            embed_t = embed_t_flat.reshape(B, T, -1) \n",
    "\n",
    "            # 2. Get history context h_t from RNN\n",
    "            h_t_seq, _ = self.rnn(embed_t)\n",
    "            # 使用 .reshape() 替代 .view() (这是导致你错误的行)\n",
    "            h_t_flat = h_t_seq.reshape(B * T, -1) \n",
    "            \n",
    "            # 3. Flatten action a_t\n",
    "            # 使用 .reshape() 替代 .view()\n",
    "            a_t_flat = a_t.reshape(B * T, -1) \n",
    "\n",
    "            # --- 4. Form Prior Distribution ---\n",
    "            # p(z_{t+1} | h_t, a_t)\n",
    "            transition_input_flat = torch.cat([h_t_flat, a_t_flat], dim=-1)\n",
    "            prior_mean_std = self.prior_net(transition_input_flat)\n",
    "            prior_mean, prior_std_pre_act = torch.chunk(prior_mean_std, 2, dim=-1)\n",
    "            prior_std = F.softplus(prior_std_pre_act) + 1e-4\n",
    "            prior_dist = Normal(prior_mean, prior_std)\n",
    "\n",
    "            # --- 5. Form Posterior Distribution ---\n",
    "            # q(z_{t+1} | h_t, a_t, s_{t+1})\n",
    "            \n",
    "            # Encode *next* observation s_next\n",
    "            # 使用 .reshape() 替代 .view()\n",
    "            s_next_flat = s_next.reshape(B * T, C, H, W) \n",
    "            embed_next_flat = self.encoder(s_next_flat) # Already normalized in encoder\n",
    "\n",
    "            posterior_input_flat = torch.cat([transition_input_flat, embed_next_flat], dim=-1)\n",
    "            post_mean_std = self.posterior_net(posterior_input_flat)\n",
    "            post_mean, post_std_pre_act = torch.chunk(post_mean_std, 2, dim=-1)\n",
    "            post_std = F.softplus(post_std_pre_act) + 1e-4\n",
    "            post_dist = Normal(post_mean, post_std)\n",
    "\n",
    "            # --- 6. Sample from Posterior for Predictions ---\n",
    "            z_next_flat = post_dist.rsample() # Reparameterization trick\n",
    "\n",
    "            # --- 7. Predict next state, reward, done ---\n",
    "            s_next_pred_flat = self.decoder(z_next_flat)\n",
    "            r_next_pred_flat = self.reward_head(z_next_flat)\n",
    "            d_next_pred_flat = self.done_head(z_next_flat)\n",
    "\n",
    "            # Reshape back to (B, T, ...)\n",
    "            # 使用 .reshape() 替代 .view()\n",
    "            s_next_pred = s_next_pred_flat.reshape(B, T, C, H, W) \n",
    "            r_next_pred = r_next_pred_flat.reshape(B, T, 1)\n",
    "            d_next_pred = d_next_pred_flat.reshape(B, T, 1)\n",
    "\n",
    "            return s_next_pred, r_next_pred, d_next_pred, post_dist, prior_dist\n",
    "    \n",
    "    def predict_from_prior(self, \n",
    "                            s_t_seq: torch.Tensor, \n",
    "                            a_t_seq: torch.Tensor\n",
    "                            ) -> tt.Tuple[torch.Tensor, ...]:\n",
    "            \"\"\"\n",
    "            Used for evaluation/visualization. \n",
    "            Predicts s_{t+1} using only the *prior* (imagination).\n",
    "            Input tensors should be (B, T, ...).\n",
    "            \"\"\"\n",
    "            B, T, C, H, W = s_t_seq.shape\n",
    "\n",
    "            # Use .reshape()\n",
    "            s_t_flat = s_t_seq.reshape(B * T, C, H, W)\n",
    "            embed_t_flat = self.encoder(s_t_flat)\n",
    "            # Use .reshape()\n",
    "            embed_t = embed_t_flat.reshape(B, T, -1)\n",
    "\n",
    "            h_t_seq, _ = self.rnn(embed_t)\n",
    "            # Use .reshape() (This was the likely source of the error)\n",
    "            h_t_flat = h_t_seq.reshape(B * T, -1)\n",
    "            \n",
    "            # Use .reshape()\n",
    "            a_t_flat = a_t_seq.reshape(B * T, -1)\n",
    "\n",
    "            transition_input_flat = torch.cat([h_t_flat, a_t_flat], dim=-1)\n",
    "            prior_mean_std = self.prior_net(transition_input_flat)\n",
    "            prior_mean, prior_std_pre_act = torch.chunk(prior_mean_std, 2, dim=-1)\n",
    "            prior_std = F.softplus(prior_std_pre_act) + 1e-4\n",
    "            \n",
    "            # Sample from the prior mean for stability during viz\n",
    "            z_next_flat = prior_mean \n",
    "\n",
    "            s_next_pred_flat = self.decoder(z_next_flat)\n",
    "            r_next_pred_flat = self.reward_head(z_next_flat)\n",
    "            d_next_pred_flat = self.done_head(z_next_flat)\n",
    "\n",
    "            # Use .reshape()\n",
    "            s_next_pred = s_next_pred_flat.reshape(B, T, C, H, W)\n",
    "            # Use .reshape()\n",
    "            r_next_pred = r_next_pred_flat.reshape(B, T, 1)\n",
    "            # Use .reshape()\n",
    "            d_next_pred = d_next_pred_flat.reshape(B, T, 1)\n",
    "\n",
    "            return s_next_pred, r_next_pred, d_next_pred\n",
    "\n",
    "\n",
    "# --- 4. Training Loop (Upgraded) ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run = wandb.init(\n",
    "        project=\"WorldModel-Training-RSSM\",\n",
    "        config=config,\n",
    "        name=config[\"RUN_NAME\"]\n",
    "    )\n",
    "    # Update config with wandb sweeps, if any\n",
    "    config = wandb.config \n",
    "\n",
    "    device = torch.device(config.DEVICE)\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    if not os.path.exists(config.CHECKPOINT_DIR):\n",
    "        os.makedirs(config.CHECKPOINT_DIR)\n",
    "\n",
    "    dataset = WorldModelDataset(config.DATA_PATH, config.CHUNK_LENGTH)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    vis_batch_size = max(1, config.BATCH_SIZE // 4)\n",
    "    vis_dataloader_iter = iter(DataLoader(\n",
    "        dataset,\n",
    "        batch_size=vis_batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        pin_memory=True\n",
    "    ))\n",
    "\n",
    "    total_action_dim = config.NUM_ACTION_BRANCHES * config.ACTION_DIM_PER_BRANCH\n",
    "    model = WorldModel(\n",
    "        input_shape=config.INPUT_SHAPE,\n",
    "        embed_dim=config.EMBED_DIM,\n",
    "        action_dim=total_action_dim,\n",
    "        rnn_hidden_dim=config.RNN_HIDDEN_DIM,\n",
    "        latent_dim=config.LATENT_DIM\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "\n",
    "    recon_loss_fn = nn.MSELoss()\n",
    "    reward_loss_fn = nn.MSELoss()\n",
    "    done_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "    wandb.watch(model, log=\"all\", log_freq=100)\n",
    "\n",
    "    # --- Checkpoint Loading Logic ---\n",
    "    start_epoch = 0\n",
    "    global_step = 0\n",
    "    \n",
    "    if config.LOAD_CHECKPOINT_PATH and os.path.exists(config.LOAD_CHECKPOINT_PATH):\n",
    "        print(f\"Loading checkpoint from: {config.LOAD_CHECKPOINT_PATH}\")\n",
    "        try:\n",
    "            checkpoint = torch.load(config.LOAD_CHECKPOINT_PATH, map_location=device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            start_epoch = checkpoint.get('epoch', 0) + 1 # Resume from the *next* epoch\n",
    "            global_step = checkpoint.get('global_step', 0)\n",
    "            print(f\"Successfully loaded checkpoint. Resuming from Epoch {start_epoch}, Global Step {global_step}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Failed to load checkpoint. {e}. Starting from scratch.\")\n",
    "            start_epoch = 0\n",
    "            global_step = 0\n",
    "    else:\n",
    "        print(\"Starting training from scratch.\")\n",
    "\n",
    "\n",
    "    for epoch in range(start_epoch, config.NUM_EPOCHS):\n",
    "        print(f\"\\n--- Epoch {epoch+1} / {config.NUM_EPOCHS} ---\")\n",
    "        model.train()\n",
    "\n",
    "        for batch in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "            s_t, a_t, r_next, d_next, s_next = batch\n",
    "            \n",
    "            s_t = s_t.to(device)\n",
    "            a_t = a_t.to(device)\n",
    "            r_next = r_next.to(device).unsqueeze(-1)\n",
    "            d_next = d_next.to(device).unsqueeze(-1)\n",
    "            s_next = s_next.to(device)\n",
    "\n",
    "            # --- Model Forward Pass ---\n",
    "            # Pass s_next to the model for posterior calculation\n",
    "            s_next_pred, r_next_pred, d_next_pred, post_dist, prior_dist = model(s_t, a_t, s_next)\n",
    "\n",
    "            # --- Target Preparation ---\n",
    "            # The decoder outputs sigmoid (0,1), so target is normalized\n",
    "            s_next_target = s_next / 255.0 \n",
    "            r_next_target = r_next\n",
    "            d_next_target = d_next.float()\n",
    "\n",
    "            # --- Loss Calculation ---\n",
    "            loss_recon = recon_loss_fn(s_next_pred, s_next_target)\n",
    "            loss_reward = reward_loss_fn(r_next_pred, r_next_target)\n",
    "            loss_done = done_loss_fn(d_next_pred, d_next_target)\n",
    "\n",
    "            # --- New KL Divergence Loss ---\n",
    "            kl_div = kl.kl_divergence(post_dist, prior_dist)\n",
    "            # Apply \"free nats\" to prevent KL from collapsing to zero\n",
    "            loss_kl_raw = kl_div.mean()\n",
    "            loss_kl = kl_div.clamp(min=config.KL_FREE_NATS).mean()\n",
    "\n",
    "            total_loss = (\n",
    "                config.RECON_LOSS_WEIGHT * loss_recon +\n",
    "                config.REWARD_LOSS_WEIGHT * loss_reward +\n",
    "                config.DONE_LOSS_WEIGHT * loss_done +\n",
    "                config.KL_LOSS_WEIGHT * loss_kl\n",
    "            )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), config.GRAD_CLIP_NORM)\n",
    "            optimizer.step()\n",
    "\n",
    "            if global_step % 100 == 0:\n",
    "                wandb.log({\n",
    "                    \"train/total_loss\": total_loss.item(),\n",
    "                    \"train/recon_loss\": loss_recon.item(),\n",
    "                    \"train/reward_loss\": loss_reward.item(),\n",
    "                    \"train/done_loss\": loss_done.item(),\n",
    "                    \"train/kl_loss\": loss_kl.item(),\n",
    "                    \"train/kl_loss_raw\": loss_kl_raw.item(),\n",
    "                    \"epoch\": epoch,\n",
    "                }, step=global_step)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        print(f\"Epoch {epoch+1} finished. Total Loss: {total_loss.item():.4f}, KL Loss: {loss_kl.item():.4f}\")\n",
    "\n",
    "        # --- Visualization ---\n",
    "        try:\n",
    "            model.eval()\n",
    "\n",
    "            try:\n",
    "                vis_s_t, vis_a_t, _, _, vis_s_next = next(vis_dataloader_iter)\n",
    "            except StopIteration:\n",
    "                vis_dataloader_iter = iter(DataLoader(\n",
    "                    dataset, batch_size=vis_batch_size, shuffle=True,\n",
    "                    num_workers=config.NUM_WORKERS, pin_memory=True\n",
    "                ))\n",
    "                vis_s_t, vis_a_t, _, _, vis_s_next = next(vis_dataloader_iter)\n",
    "\n",
    "            vis_s_t = vis_s_t.to(device)\n",
    "            vis_a_t = vis_a_t.to(device)\n",
    "            vis_s_next = vis_s_next.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Use the \"imagination\" (prior) for visualization\n",
    "                vis_s_next_pred, _, _ = model.predict_from_prior(vis_s_t, vis_a_t)\n",
    "\n",
    "            s_next_target = (vis_s_next[0] / 255.0).cpu().detach()\n",
    "            s_next_pred_vis = vis_s_next_pred[0].cpu().detach()\n",
    "\n",
    "            comparison_video_tensor = torch.cat(\n",
    "                [s_next_target, s_next_pred_vis], dim=3\n",
    "            ).clamp(0, 1)\n",
    "\n",
    "            if comparison_video_tensor.shape[1] == 1:\n",
    "                comparison_video_tensor = comparison_video_tensor.expand(-1, 3, -1, -1)\n",
    "            \n",
    "            video_data = (comparison_video_tensor * 255).to(torch.uint8)\n",
    "\n",
    "            wandb.log({\n",
    "                \"eval/prediction_video\": wandb.Video(\n",
    "                    video_data,\n",
    "                    fps=8,\n",
    "                    caption=\"Left: Ground Truth (s_t+1), Right: Prediction (from prior)\",\n",
    "                    format=\"gif\"\n",
    "                )\n",
    "            }, step=global_step)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to log video: {e}\")\n",
    "        finally:\n",
    "            model.train()\n",
    "\n",
    "        # --- Checkpoint Saving ---\n",
    "        checkpoint_data = {\n",
    "            'epoch': epoch,\n",
    "            'global_step': global_step,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "        }\n",
    "        \n",
    "        # Save epoch-specific checkpoint\n",
    "        save_path_epoch = os.path.join(config.CHECKPOINT_DIR, f\"wm_epoch_{epoch+1}.pth\")\n",
    "        torch.save(checkpoint_data, save_path_epoch)\n",
    "        \n",
    "        # Save latest checkpoint\n",
    "        save_path_latest = os.path.join(config.CHECKPOINT_DIR, \"latest_checkpoint.pth\")\n",
    "        torch.save(checkpoint_data, save_path_latest)\n",
    "        \n",
    "        print(f\"Model saved to: {save_path_latest}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpickleball",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
