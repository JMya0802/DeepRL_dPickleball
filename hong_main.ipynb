{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fc2746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tan04\\anaconda3\\envs\\dpickleball\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhongttisme\u001b[0m (\u001b[33mhongttisme-xiamen-university-malaysia\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\tan04\\Documents\\CodePlace\\AI\\pickleBall\\PPO_Skip_N_Stacking\\wandb\\run-20251013_074909-67k7fgjv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hongttisme-xiamen-university-malaysia/PPO-Skip-N-Step-Parallel-Resumable/runs/67k7fgjv' target=\"_blank\">2ez2n4af</a></strong> to <a href='https://wandb.ai/hongttisme-xiamen-university-malaysia/PPO-Skip-N-Step-Parallel-Resumable' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hongttisme-xiamen-university-malaysia/PPO-Skip-N-Step-Parallel-Resumable' target=\"_blank\">https://wandb.ai/hongttisme-xiamen-university-malaysia/PPO-Skip-N-Step-Parallel-Resumable</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hongttisme-xiamen-university-malaysia/PPO-Skip-N-Step-Parallel-Resumable/runs/67k7fgjv' target=\"_blank\">https://wandb.ai/hongttisme-xiamen-university-malaysia/PPO-Skip-N-Step-Parallel-Resumable/runs/67k7fgjv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 8 environments...\n",
      "  - Env 0 started on port 5005.\n",
      "  - Env 1 started on port 5015.\n",
      "  - Env 2 started on port 5025.\n",
      "  - Env 3 started on port 5035.\n",
      "  - Env 4 started on port 5045.\n",
      "  - Env 5 started on port 5055.\n",
      "  - Env 6 started on port 5065.\n",
      "  - Env 7 started on port 5075.\n",
      "Detected observation shape from environment: (4, 54, 132)\n",
      "Created checkpoint directory: saved_models_ppo_dual_resnet_skip_4\\2ez2n4af\n",
      "No checkpoint found. Starting a new training run.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PPO Updates:   0%|          | 1/1220 [01:10<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving final checkpoint and closing all environments...\n",
      "Saving checkpoint to saved_models_ppo_dual_resnet_skip_4\\2ez2n4af\\checkpoint.pth at frame 0...\n",
      "Checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">2ez2n4af</strong> at: <a href='https://wandb.ai/hongttisme-xiamen-university-malaysia/PPO-Skip-N-Step-Parallel-Resumable/runs/67k7fgjv' target=\"_blank\">https://wandb.ai/hongttisme-xiamen-university-malaysia/PPO-Skip-N-Step-Parallel-Resumable/runs/67k7fgjv</a><br> View project at: <a href='https://wandb.ai/hongttisme-xiamen-university-malaysia/PPO-Skip-N-Step-Parallel-Resumable' target=\"_blank\">https://wandb.ai/hongttisme-xiamen-university-malaysia/PPO-Skip-N-Step-Parallel-Resumable</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251013_074909-67k7fgjv\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleanup complete. Final checkpoint saved.\n"
     ]
    },
    {
     "ename": "UnityCommunicatorStoppedException",
     "evalue": "Communicator has exited.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnityCommunicatorStoppedException\u001b[0m         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 508\u001b[0m\n\u001b[0;32m    505\u001b[0m     optimizer_p0\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m new_lr\n\u001b[0;32m    506\u001b[0m     optimizer_p1\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m new_lr\n\u001b[1;32m--> 508\u001b[0m trajectories_p0_by_env, trajectories_p1_by_env, last_states \u001b[38;5;241m=\u001b[39m \u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_trajectories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_p0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_p1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m frames_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(traj_list) \u001b[38;5;28;01mfor\u001b[39;00m traj_list \u001b[38;5;129;01min\u001b[39;00m trajectories_p0_by_env\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;241m*\u001b[39m config\u001b[38;5;241m.\u001b[39mSKIP_N\n\u001b[0;32m    511\u001b[0m total_frames \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m frames_in_batch\n",
      "Cell \u001b[1;32mIn[1], line 297\u001b[0m, in \u001b[0;36mCollector.collect_trajectories\u001b[1;34m(self, net_p0, net_p1, device)\u001b[0m\n\u001b[0;32m    294\u001b[0m     log_probs_p1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(d\u001b[38;5;241m.\u001b[39mlog_prob(a) \u001b[38;5;28;01mfor\u001b[39;00m d, a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dists_p1, actions_p1))\n\u001b[0;32m    295\u001b[0m     action_idxs_p1 \u001b[38;5;241m=\u001b[39m actions_p1[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m*\u001b[39m actions_p1[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m9\u001b[39m \u001b[38;5;241m*\u001b[39m actions_p1[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m--> 297\u001b[0m new_states, rewards_dict, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_idxs_p0\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_idxs_p1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mNUM_ENVS):\n\u001b[0;32m    303\u001b[0m     trajectories_p0[i]\u001b[38;5;241m.\u001b[39mappend(TrajectoryItem(\n\u001b[0;32m    304\u001b[0m         state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_states[i], action\u001b[38;5;241m=\u001b[39maction_idxs_p0[i]\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m    305\u001b[0m         reward\u001b[38;5;241m=\u001b[39mrewards_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menvs\u001b[38;5;241m.\u001b[39mp0_id][i], done\u001b[38;5;241m=\u001b[39mdones[i],\n\u001b[0;32m    306\u001b[0m         log_prob\u001b[38;5;241m=\u001b[39mlog_probs_p0[i]\u001b[38;5;241m.\u001b[39mitem(), value\u001b[38;5;241m=\u001b[39mvalues_p0_v[i]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    307\u001b[0m     ))\n",
      "Cell \u001b[1;32mIn[1], line 237\u001b[0m, in \u001b[0;36mMulti_Env_With_SkipN_and_Stack.step\u001b[1;34m(self, action_idxs_p0, action_idxs_p1)\u001b[0m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dones[i]:\n\u001b[0;32m    236\u001b[0m     env_actions \u001b[38;5;241m=\u001b[39m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp0_id: decoded_p0_actions[i], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp1_id: decoded_p1_actions[i]}\n\u001b[1;32m--> 237\u001b[0m     new_frame, rewards, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m     cum_p0_rewards[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp0_id] \u001b[38;5;241m-\u001b[39m rewards[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp1_id]\n\u001b[0;32m    239\u001b[0m     cum_p1_rewards[i] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp1_id] \u001b[38;5;241m-\u001b[39m rewards[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp0_id]\n",
      "Cell \u001b[1;32mIn[1], line 169\u001b[0m, in \u001b[0;36mUnityEnvWrapper.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, actions: \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tt\u001b[38;5;241m.\u001b[39mTuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m--> 169\u001b[0m     observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     img \u001b[38;5;241m=\u001b[39m observations[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp0_id][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobservation\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    171\u001b[0m     processed_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_obs(img)\n",
      "File \u001b[1;32mc:\\users\\tan04\\documents\\codeplace\\ai\\pickleball\\dpickleball-ml-agents\\ml-agents-envs\\mlagents_envs\\envs\\unity_parallel_env.py:47\u001b[0m, in \u001b[0;36mUnityParallelEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rewards[k] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Step environment\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Agent cleanup and sorting\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cleanup_agents()\n",
      "File \u001b[1;32mc:\\users\\tan04\\documents\\codeplace\\ai\\pickleball\\dpickleball-ml-agents\\ml-agents-envs\\mlagents_envs\\envs\\unity_pettingzoo_base_env.py:194\u001b[0m, in \u001b[0;36mUnityPettingzooBaseEnv._step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mset_actions(behavior_name, actions)\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# print(\"behavior_name, actions\",behavior_name, actions)\u001b[39;00m\n\u001b[1;32m--> 194\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_states()\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m behavior_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_env\u001b[38;5;241m.\u001b[39mbehavior_specs\u001b[38;5;241m.\u001b[39mkeys():\n",
      "File \u001b[1;32mc:\\users\\tan04\\documents\\codeplace\\ai\\pickleball\\dpickleball-ml-agents\\ml-agents-envs\\mlagents_envs\\timers.py:305\u001b[0m, in \u001b[0;36mtimed.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hierarchical_timer(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m):\n\u001b[1;32m--> 305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\users\\tan04\\documents\\codeplace\\ai\\pickleball\\dpickleball-ml-agents\\ml-agents-envs\\mlagents_envs\\environment.py:350\u001b[0m, in \u001b[0;36mUnityEnvironment.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    348\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_communicator\u001b[38;5;241m.\u001b[39mexchange(step_input, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll_process)\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnityCommunicatorStoppedException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCommunicator has exited.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_behavior_specs(outputs)\n\u001b[0;32m    352\u001b[0m rl_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mrl_output\n",
      "\u001b[1;31mUnityCommunicatorStoppedException\u001b[0m: Communicator has exited."
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import time\n",
    "import typing as tt\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv\n",
    "from mlagents_envs.envs.custom_side_channel import CustomDataChannel, StringSideChannel\n",
    "import wandb\n",
    "\n",
    "# Initialize Weights & Biases for experiment tracking\n",
    "# ### MODIFIED: wandb.init is now called later, after we check for a checkpoint.\n",
    "# ### This allows us to resume a specific run ID.\n",
    "\n",
    "config_dict = {\n",
    "    # --- PPO & GAE ---\n",
    "    \"GAMMA\"              : 0.99,\n",
    "    \"GAE_LAMBDA\"         : 0.95,\n",
    "    \"PPO_EPSILON\"        : 0.2,\n",
    "    \"PPO_EPOCHS\"         : 4,\n",
    "    \"PPO_BATCH_SIZE\"     : 1024,\n",
    "    \"ENTROPY_COEF\"       : 0.01,\n",
    "    \"VALUE_LOSS_COEF\"    : 0.5,\n",
    "    \"LEARNING_RATE\"      : 2.5e-4,\n",
    "    \"LR_SCHEDULE_ANNEAL\" : True,\n",
    "    \"TOTAL_TIMESTEPS\"    : 10_000_000,\n",
    "\n",
    "    # --- Environment & Sampling ---\n",
    "    \"SKIP_N\"             : 4,\n",
    "    \"STACK_N\"            : 4,\n",
    "    \"MAX_EPISODE_STEPS\"  : 2000,\n",
    "    \"NUM_ENVS\"           : 8,\n",
    "    \"PPO_STEPS_PER_ENV\"  : 1024,\n",
    "    \"BASE_PORT\"          : 5005,\n",
    "    \"PORT_STRIDE\"        : 10,\n",
    "    \"EXECUTABLE_PATH\"    : r\"C:\\Users\\tan04\\Downloads\\JMPickleBallFinal\\Training Version\\dp.exe\",\n",
    "\n",
    "    # --- Model Management & Evaluation ---\n",
    "    \"SAVE_EVERY_FRAMES\"  : 100_000,\n",
    "    \"MODEL_SAVE_PATH\"    : \"saved_models_ppo_dual_resnet_skip_4\",\n",
    "}\n",
    "\n",
    "\n",
    "### NEW CLASS: CheckpointManager to handle saving and loading the complete training state ###\n",
    "class CheckpointManager:\n",
    "    \"\"\"Handles saving and loading of the entire training state for resuming.\"\"\"\n",
    "    def __init__(self, base_model_path: str, run_name: str):\n",
    "        self.checkpoint_dir = os.path.join(base_model_path, run_name)\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, \"checkpoint.pth\")\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "            print(f\"Created checkpoint directory: {self.checkpoint_dir}\")\n",
    "\n",
    "    def save_checkpoint(self, net_p0: nn.Module, net_p1: nn.Module, \n",
    "                        optimizer_p0: optim.Optimizer, optimizer_p1: optim.Optimizer,\n",
    "                        total_frames: int, update: int, wandb_run_id: str):\n",
    "        \"\"\"Saves the complete state needed to resume training.\"\"\"\n",
    "        state = {\n",
    "            'net_p0_state_dict': net_p0.state_dict(),\n",
    "            'net_p1_state_dict': net_p1.state_dict(),\n",
    "            'optimizer_p0_state_dict': optimizer_p0.state_dict(),\n",
    "            'optimizer_p1_state_dict': optimizer_p1.state_dict(),\n",
    "            'total_frames': total_frames,\n",
    "            'update': update,\n",
    "            'wandb_run_id': wandb_run_id\n",
    "        }\n",
    "        print(f\"Saving checkpoint to {self.checkpoint_file} at frame {total_frames}...\")\n",
    "        torch.save(state, self.checkpoint_file)\n",
    "        print(\"Checkpoint saved.\")\n",
    "\n",
    "    def load_checkpoint(self, device: torch.device) -> tt.Optional[dict]:\n",
    "        \"\"\"Loads a checkpoint if it exists.\"\"\"\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            print(f\"Loading checkpoint from {self.checkpoint_file}...\")\n",
    "            checkpoint = torch.load(self.checkpoint_file, map_location=device)\n",
    "            print(\"Checkpoint loaded.\")\n",
    "            return checkpoint\n",
    "        else:\n",
    "            print(\"No checkpoint found. Starting a new training run.\")\n",
    "            return None\n",
    "\n",
    "# --- The rest of the classes are unchanged ---\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"A simple residual block with two convolutional layers.\"\"\"\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        out += residual\n",
    "        return self.relu(out)\n",
    "\n",
    "class ActorCriticResNet(nn.Module):\n",
    "    \"\"\"An upgraded Actor-Critic network using residual blocks.\"\"\"\n",
    "    def __init__(self, input_shape: tt.Tuple[int, int, int], n_actions_per_head: tt.List[int]):\n",
    "        super().__init__()\n",
    "        c, h, w = input_shape\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
    "            ResidualBlock(64),\n",
    "            ResidualBlock(64),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Calculate the size of the flattened features\n",
    "        with torch.no_grad():\n",
    "            conv_out_size = self.conv(torch.zeros(1, *input_shape)).size(-1)\n",
    "            \n",
    "        # Shared MLP base\n",
    "        self.fc_base = nn.Sequential(nn.Linear(conv_out_size, 512), nn.ReLU())\n",
    "        \n",
    "        # Policy heads for multi-discrete actions\n",
    "        self.action_head_0 = nn.Linear(512, n_actions_per_head[0])\n",
    "        self.action_head_1 = nn.Linear(512, n_actions_per_head[1])\n",
    "        self.action_head_2 = nn.Linear(512, n_actions_per_head[2])\n",
    "        \n",
    "        # Value head\n",
    "        self.value_head = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tt.Tuple[tt.Tuple[torch.Tensor, ...], torch.Tensor]:\n",
    "        base_features = self.fc_base(self.conv(x.float() / 255.0)) # Normalize pixels\n",
    "        logits = (\n",
    "            self.action_head_0(base_features), \n",
    "            self.action_head_1(base_features), \n",
    "            self.action_head_2(base_features)\n",
    "        )\n",
    "        value = self.value_head(base_features)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "class UnityEnvWrapper:\n",
    "    \"\"\"A wrapper for a single UnityParallelEnv to standardize interactions.\"\"\"\n",
    "    def __init__(self, env: UnityParallelEnv, max_episode_steps: int):\n",
    "        self.env = env\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "        self.agent_ids = self.env.agents\n",
    "        self.p0_id, self.p1_id = self.agent_ids[0], self.agent_ids[1]\n",
    "        self._step_count = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess_obs(img: np.ndarray) -> np.ndarray:\n",
    "        # Convert to grayscale and crop. Result is (1, H, W)\n",
    "        gray = 0.299 * img[0] + 0.587 * img[1] + 0.114 * img[2]\n",
    "        return (gray[np.newaxis, 24:-6, 18:-18] * 255).astype(np.uint8)\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self._step_count = 0\n",
    "        observations = self.env.reset()\n",
    "        img = observations[self.p0_id]['observation'][0]\n",
    "        return self._preprocess_obs(img)\n",
    "\n",
    "    def step(self, actions: dict) -> tt.Tuple[np.ndarray, dict, bool, dict]:\n",
    "        observations, rewards, dones, infos = self.env.step(actions)\n",
    "        img = observations[self.p0_id]['observation'][0]\n",
    "        processed_frame = self._preprocess_obs(img)\n",
    "        step_rewards = {self.p0_id: float(rewards.get(self.p0_id, 0.0)), self.p1_id: float(rewards.get(self.p1_id, 0.0))}\n",
    "        done_env = bool(dones.get('__all__', False))\n",
    "        self._step_count += 1\n",
    "        truncated = self._step_count >= self.max_episode_steps\n",
    "        done = done_env or truncated\n",
    "        return processed_frame, step_rewards, done, {}\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "class Multi_Env_With_SkipN_and_Stack:\n",
    "    \"\"\"Manages multiple parallel environments with frame skipping and stacking.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.envs: tt.List[UnityEnvWrapper] = []\n",
    "        self._frame_stacks: tt.List[collections.deque] = []\n",
    "        \n",
    "        print(f\"Starting {self.config.NUM_ENVS} environments...\")\n",
    "        for i in range(self.config.NUM_ENVS):\n",
    "            port = self.config.BASE_PORT + i * self.config.PORT_STRIDE\n",
    "            channel = CustomDataChannel()\n",
    "            string_channel = StringSideChannel()\n",
    "            channel.send_data(serve=212, p1=0, p2=0) \n",
    "            unity_env = UnityEnvironment(file_name=self.config.EXECUTABLE_PATH, base_port=port, side_channels=[string_channel, channel])\n",
    "            wrapped_env = UnityEnvWrapper(UnityParallelEnv(unity_env), self.config.MAX_EPISODE_STEPS)\n",
    "            self.envs.append(wrapped_env)\n",
    "            self._frame_stacks.append(collections.deque(maxlen=self.config.STACK_N))\n",
    "            print(f\"  - Env {i} started on port {port}.\")\n",
    "\n",
    "        first_env = self.envs[0]\n",
    "        self.p0_id, self.p1_id = first_env.p0_id, first_env.p1_id\n",
    "        \n",
    "        self.states = self.reset()\n",
    "        \n",
    "    def _get_env_state(self, frame_stack: collections.deque) -> np.ndarray:\n",
    "        return np.concatenate(list(frame_stack), axis=0)\n",
    "\n",
    "    def reset(self) -> tt.List[np.ndarray]:\n",
    "        for i, env in enumerate(self.envs):\n",
    "            first_frame = env.reset()\n",
    "            for _ in range(self.config.STACK_N):\n",
    "                self._frame_stacks[i].append(first_frame)\n",
    "        return [self._get_env_state(fs) for fs in self._frame_stacks]\n",
    "\n",
    "    @staticmethod\n",
    "    def _decode_action(action_index: int) -> np.ndarray:\n",
    "        a0 = action_index % 3\n",
    "        a1 = (action_index // 3) % 3\n",
    "        a2 = (action_index // 9) % 3\n",
    "        return np.array([a0, a1, a2], dtype=np.int32)\n",
    "\n",
    "    def step(self, action_idxs_p0: tt.List[int], action_idxs_p1: tt.List[int]) -> tt.Tuple[tt.List[np.ndarray], dict, tt.List[bool]]:\n",
    "        num_envs = len(self.envs)\n",
    "        cum_p0_rewards = [0.0] * num_envs\n",
    "        cum_p1_rewards = [0.0] * num_envs\n",
    "        dones = [False] * num_envs\n",
    "        last_frames = [None] * num_envs\n",
    "\n",
    "        decoded_p0_actions = [self._decode_action(idx) for idx in action_idxs_p0]\n",
    "        decoded_p1_actions = [self._decode_action(idx) for idx in action_idxs_p1]\n",
    "\n",
    "        for _ in range(self.config.SKIP_N):\n",
    "            for i, env in enumerate(self.envs):\n",
    "                if not dones[i]:\n",
    "                    env_actions = {self.p0_id: decoded_p0_actions[i], self.p1_id: decoded_p1_actions[i]}\n",
    "                    new_frame, rewards, done, _ = env.step(env_actions)\n",
    "                    cum_p0_rewards[i] += rewards[self.p0_id] - rewards[self.p1_id]\n",
    "                    cum_p1_rewards[i] += rewards[self.p1_id] - rewards[self.p0_id]\n",
    "                    last_frames[i] = new_frame\n",
    "                    if done:\n",
    "                        dones[i] = True\n",
    "        \n",
    "        for i in range(num_envs):\n",
    "            if dones[i]:\n",
    "                first_frame = self.envs[i].reset()\n",
    "                self._frame_stacks[i].clear()\n",
    "                for _ in range(self.config.STACK_N):\n",
    "                    self._frame_stacks[i].append(first_frame)\n",
    "            else:\n",
    "                self._frame_stacks[i].append(last_frames[i])\n",
    "        \n",
    "        new_states = [self._get_env_state(fs) for fs in self._frame_stacks]\n",
    "        reward_dict = {self.p0_id: cum_p0_rewards, self.p1_id: cum_p1_rewards}\n",
    "        \n",
    "        return new_states, reward_dict, dones\n",
    "\n",
    "    def close(self):\n",
    "        for env in self.envs: env.close()\n",
    "\n",
    "# --- PPO Implementation ---\n",
    "@dataclass\n",
    "class TrajectoryItem:\n",
    "    state: np.ndarray\n",
    "    action: int\n",
    "    reward: float\n",
    "    done: bool\n",
    "    log_prob: float\n",
    "    value: float\n",
    "\n",
    "class Collector:\n",
    "    def __init__(self, envs: Multi_Env_With_SkipN_and_Stack, config):\n",
    "        self.envs = envs\n",
    "        self.config = config\n",
    "        self.current_states = self.envs.states\n",
    "\n",
    "    def collect_trajectories(self, net_p0: ActorCriticResNet, net_p1: ActorCriticResNet, device: torch.device) -> tt.Tuple[dict, dict, np.ndarray]:\n",
    "        trajectories_p0 = {i: [] for i in range(self.config.NUM_ENVS)}\n",
    "        trajectories_p1 = {i: [] for i in range(self.config.NUM_ENVS)}\n",
    "        \n",
    "        for _ in range(self.config.PPO_STEPS_PER_ENV):\n",
    "            states_v = torch.from_numpy(np.array(self.current_states)).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits_p0, values_p0_v = net_p0(states_v)\n",
    "                dists_p0 = [Categorical(logits=l) for l in logits_p0]\n",
    "                actions_p0 = [d.sample() for d in dists_p0]\n",
    "                log_probs_p0 = sum(d.log_prob(a) for d, a in zip(dists_p0, actions_p0))\n",
    "                action_idxs_p0 = actions_p0[0] + 3 * actions_p0[1] + 9 * actions_p0[2]\n",
    "\n",
    "                logits_p1, values_p1_v = net_p1(states_v)\n",
    "                dists_p1 = [Categorical(logits=l) for l in logits_p1]\n",
    "                actions_p1 = [d.sample() for d in dists_p1]\n",
    "                log_probs_p1 = sum(d.log_prob(a) for d, a in zip(dists_p1, actions_p1))\n",
    "                action_idxs_p1 = actions_p1[0] + 3 * actions_p1[1] + 9 * actions_p1[2]\n",
    "\n",
    "            new_states, rewards_dict, dones = self.envs.step(\n",
    "                action_idxs_p0.cpu().numpy(), \n",
    "                action_idxs_p1.cpu().numpy()\n",
    "            )\n",
    "            \n",
    "            for i in range(self.config.NUM_ENVS):\n",
    "                trajectories_p0[i].append(TrajectoryItem(\n",
    "                    state=self.current_states[i], action=action_idxs_p0[i].item(),\n",
    "                    reward=rewards_dict[self.envs.p0_id][i], done=dones[i],\n",
    "                    log_prob=log_probs_p0[i].item(), value=values_p0_v[i].item()\n",
    "                ))\n",
    "                trajectories_p1[i].append(TrajectoryItem(\n",
    "                    state=self.current_states[i], action=action_idxs_p1[i].item(),\n",
    "                    reward=rewards_dict[self.envs.p1_id][i], done=dones[i],\n",
    "                    log_prob=log_probs_p1[i].item(), value=values_p1_v[i].item()\n",
    "                ))\n",
    "            \n",
    "            self.current_states = new_states\n",
    "        \n",
    "        last_states = np.array(self.current_states)\n",
    "        return trajectories_p0, trajectories_p1, last_states\n",
    "\n",
    "class PPOTrainer:\n",
    "    def __init__(self, net: ActorCriticResNet, optimizer: optim.Optimizer, device: torch.device, agent_id: str, config):\n",
    "        self.net = net\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.agent_id = agent_id\n",
    "        self.config = config\n",
    "\n",
    "    def _calculate_advantages_and_returns(self, trajectories_by_env: tt.Dict[int, tt.List[TrajectoryItem]], last_states: np.ndarray):\n",
    "        all_advantages, all_returns = [], []\n",
    "        last_states_v = torch.from_numpy(last_states).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            _, last_vals_v = self.net(last_states_v)\n",
    "        \n",
    "        for i in range(len(trajectories_by_env)):\n",
    "            advantages, returns = [], []\n",
    "            gae_advantage = 0.0\n",
    "            next_value = last_vals_v[i].item()\n",
    "            if trajectories_by_env[i][-1].done:\n",
    "                next_value = 0.0\n",
    "            \n",
    "            for traj in reversed(trajectories_by_env[i]):           \n",
    "                delta = traj.reward + self.config.GAMMA * next_value * (1 - int(traj.done)) - traj.value\n",
    "                gae_advantage = delta + self.config.GAMMA * self.config.GAE_LAMBDA * gae_advantage * (1 - int(traj.done))\n",
    "                returns.append(gae_advantage + traj.value)\n",
    "                advantages.append(gae_advantage)\n",
    "                next_value = traj.value\n",
    "            \n",
    "            all_advantages.extend(list(reversed(advantages)))\n",
    "            all_returns.extend(list(reversed(returns)))\n",
    "\n",
    "        return all_advantages, all_returns\n",
    "\n",
    "    def train(self, trajectories_by_env: tt.Dict[int, tt.List[TrajectoryItem]], last_states: np.ndarray):\n",
    "        trajectories = [item for sublist in trajectories_by_env.values() for item in sublist]\n",
    "        advantages, returns = self._calculate_advantages_and_returns(trajectories_by_env, last_states)\n",
    "        \n",
    "        states = np.array([t.state for t in trajectories])\n",
    "        actions = np.array([t.action for t in trajectories])\n",
    "        log_probs_old = np.array([t.log_prob for t in trajectories], dtype=np.float32)\n",
    "        advantages = np.array(advantages, dtype=np.float32)\n",
    "        returns = np.array(returns, dtype=np.float32)\n",
    "        values_old = np.array([t.value for t in trajectories], dtype=np.float32)\n",
    "\n",
    "        var_y = np.var(returns)\n",
    "        explained_var = 1 - np.var(returns - values_old) / (var_y + 1e-8) if var_y > 1e-8 else 0\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        for _ in range(self.config.PPO_EPOCHS):\n",
    "            indices = np.random.permutation(len(trajectories))\n",
    "            for start in range(0, len(trajectories), self.config.PPO_BATCH_SIZE):\n",
    "                end = start + self.config.PPO_BATCH_SIZE\n",
    "                batch_indices = indices[start:end]\n",
    "                \n",
    "                batch_states = torch.from_numpy(states[batch_indices]).to(self.device)\n",
    "                batch_actions_v = torch.from_numpy(actions[batch_indices]).to(self.device)\n",
    "                batch_log_probs_old_v = torch.from_numpy(log_probs_old[batch_indices]).to(self.device)\n",
    "                batch_advantages_v = torch.from_numpy(advantages[batch_indices]).to(self.device)\n",
    "                batch_returns_v = torch.from_numpy(returns[batch_indices]).to(self.device)\n",
    "                \n",
    "                logits, values_v = self.net(batch_states)\n",
    "                values_v = values_v.squeeze(-1)\n",
    "\n",
    "                dists = [Categorical(logits=l) for l in logits]\n",
    "                a0 = batch_actions_v % 3\n",
    "                a1 = (batch_actions_v // 3) % 3\n",
    "                a2 = (batch_actions_v // 9) % 3\n",
    "                log_probs_new = dists[0].log_prob(a0) + dists[1].log_prob(a1) + dists[2].log_prob(a2)\n",
    "                \n",
    "                ratio = torch.exp(log_probs_new - batch_log_probs_old_v)\n",
    "                surr1 = ratio * batch_advantages_v\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.config.PPO_EPSILON, 1.0 + self.config.PPO_EPSILON) * batch_advantages_v\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                value_loss = nn.functional.mse_loss(values_v, batch_returns_v)\n",
    "                entropy = sum(d.entropy().mean() for d in dists)\n",
    "                \n",
    "                total_loss = (policy_loss + self.config.VALUE_LOSS_COEF * value_loss - self.config.ENTROPY_COEF * entropy)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=0.5)\n",
    "                self.optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            log_ratio = log_probs_new - batch_log_probs_old_v\n",
    "            approx_kl = torch.mean((torch.exp(log_ratio) - 1) - log_ratio).item()\n",
    "            clip_fraction = torch.mean((torch.abs(ratio - 1.0) > self.config.PPO_EPSILON).float()).item()\n",
    "        \n",
    "        wandb.log({\n",
    "            f\"train/{self.agent_id}_policy_loss\": policy_loss.item(),\n",
    "            f\"train/{self.agent_id}_value_loss\": value_loss.item(),\n",
    "            f\"train/{self.agent_id}_entropy\": entropy.item(),\n",
    "            f\"train/{self.agent_id}_explained_variance\": explained_var,\n",
    "            f\"train/{self.agent_id}_approx_kl\": approx_kl,\n",
    "            f\"train/{self.agent_id}_clip_fraction\": clip_fraction,\n",
    "        }, commit=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # --- Initialization ---\n",
    "    ### MODIFIED: Generate a potential run name first. This will be used for the checkpoint directory.\n",
    "    # We will use this or the one from wandb to ensure consistency.\n",
    "    run_name = wandb.util.generate_id() \n",
    "    \n",
    "    # Initialize wandb and config object\n",
    "    run = wandb.init(\n",
    "        project=\"PPO-Skip-N-Step-Parallel-Resumable\",\n",
    "        config=config_dict,\n",
    "        name=run_name, # Use a temporary name\n",
    "        # ### MODIFIED: We will re-init or resume later\n",
    "        # id=...\n",
    "        # resume=...\n",
    "    )\n",
    "    # The config object is now accessible via run.config\n",
    "    config = run.config\n",
    "\n",
    "    envs = Multi_Env_With_SkipN_and_Stack(config)\n",
    "    actual_input_shape = envs.states[0].shape\n",
    "    print(f\"Detected observation shape from environment: {actual_input_shape}\")\n",
    "    \n",
    "    n_actions_per_head = [3, 3, 3]\n",
    "\n",
    "    net_p0 = ActorCriticResNet(actual_input_shape, n_actions_per_head).to(device)\n",
    "    net_p1 = ActorCriticResNet(actual_input_shape, n_actions_per_head).to(device)\n",
    "    \n",
    "    optimizer_p0 = optim.Adam(net_p0.parameters(), lr=config.LEARNING_RATE, eps=1e-5)\n",
    "    optimizer_p1 = optim.Adam(net_p1.parameters(), lr=config.LEARNING_RATE, eps=1e-5)\n",
    "    \n",
    "    ### MODIFIED: Using the new CheckpointManager ###\n",
    "    # Use wandb's official run name for the directory to keep things tidy\n",
    "    checkpoint_manager = CheckpointManager(config.MODEL_SAVE_PATH, wandb.run.name)\n",
    "    checkpoint = checkpoint_manager.load_checkpoint(device)\n",
    "\n",
    "    # State variables that will be loaded from checkpoint if it exists\n",
    "    total_frames = 0\n",
    "    start_update = 1\n",
    "    last_save_frame = 0\n",
    "    wandb_run_id = wandb.run.id # Get the ID of the new run by default\n",
    "\n",
    "    if checkpoint:\n",
    "        print(\"Resuming training from checkpoint...\")\n",
    "        net_p0.load_state_dict(checkpoint['net_p0_state_dict'])\n",
    "        net_p1.load_state_dict(checkpoint['net_p1_state_dict'])\n",
    "        optimizer_p0.load_state_dict(checkpoint['optimizer_p0_state_dict'])\n",
    "        optimizer_p1.load_state_dict(checkpoint['optimizer_p1_state_dict'])\n",
    "        total_frames = checkpoint['total_frames']\n",
    "        start_update = checkpoint['update'] + 1\n",
    "        wandb_run_id = checkpoint['wandb_run_id']\n",
    "        last_save_frame = total_frames\n",
    "        \n",
    "        # ### MODIFIED: Finish the temporary run and start a new one resuming the old ID\n",
    "        wandb.finish()\n",
    "        run = wandb.init(\n",
    "            project=\"PPO-Skip-N-Step-Parallel-Resumable\",\n",
    "            config=config_dict,\n",
    "            id=wandb_run_id,\n",
    "            resume=\"must\" # Use \"must\" to ensure it resumes correctly\n",
    "        )\n",
    "        config = run.config # Re-assign config from the resumed run\n",
    "        print(f\"Resumed W&B run: {wandb_run_id}\")\n",
    "\n",
    "    collector = Collector(envs, config)\n",
    "    trainer_p0 = PPOTrainer(net_p0, optimizer_p0, device, agent_id=\"p0\", config=config)\n",
    "    trainer_p1 = PPOTrainer(net_p1, optimizer_p1, device, agent_id=\"p1\", config=config)\n",
    "    \n",
    "    ep_info_buffer_p0 = collections.deque(maxlen=100)\n",
    "    ep_info_buffer_p1 = collections.deque(maxlen=100)\n",
    "    ep_len_buffer = collections.deque(maxlen=100)\n",
    "    \n",
    "    current_ep_rewards_p0 = np.zeros(config.NUM_ENVS, dtype=np.float32)\n",
    "    current_ep_rewards_p1 = np.zeros(config.NUM_ENVS, dtype=np.float32)\n",
    "    current_ep_lengths = np.zeros(config.NUM_ENVS, dtype=np.int32)\n",
    "    \n",
    "    # --- Main Training Loop ---\n",
    "    start_time = time.time()\n",
    "    num_updates = config.TOTAL_TIMESTEPS // (config.NUM_ENVS * config.PPO_STEPS_PER_ENV)\n",
    "    \n",
    "    try:\n",
    "        ### MODIFIED: Start the loop from the correct update number ###\n",
    "        for update in tqdm(range(start_update, num_updates + 1), desc=\"PPO Updates\", initial=start_update, total=num_updates):\n",
    "            if config.LR_SCHEDULE_ANNEAL:\n",
    "                frac = 1.0 - (update - 1.0) / num_updates\n",
    "                new_lr = config.LEARNING_RATE * frac\n",
    "                optimizer_p0.param_groups[0][\"lr\"] = new_lr\n",
    "                optimizer_p1.param_groups[0][\"lr\"] = new_lr\n",
    "\n",
    "            trajectories_p0_by_env, trajectories_p1_by_env, last_states = collector.collect_trajectories(net_p0, net_p1, device)\n",
    "            \n",
    "            frames_in_batch = sum(len(traj_list) for traj_list in trajectories_p0_by_env.values()) * config.SKIP_N\n",
    "            total_frames += frames_in_batch\n",
    "            \n",
    "            for i in range(config.NUM_ENVS):\n",
    "                for t_item_p0, t_item_p1 in zip(trajectories_p0_by_env[i], trajectories_p1_by_env[i]):\n",
    "                    current_ep_rewards_p0[i] += t_item_p0.reward\n",
    "                    current_ep_rewards_p1[i] += t_item_p1.reward\n",
    "                    current_ep_lengths[i] += 1\n",
    "                    \n",
    "                    if t_item_p0.done:\n",
    "                        ep_info_buffer_p0.append(current_ep_rewards_p0[i])\n",
    "                        ep_info_buffer_p1.append(current_ep_rewards_p1[i])\n",
    "                        ep_len_buffer.append(current_ep_lengths[i])\n",
    "                        \n",
    "                        current_ep_rewards_p0[i] = 0\n",
    "                        current_ep_rewards_p1[i] = 0\n",
    "                        current_ep_lengths[i] = 0\n",
    "\n",
    "            trainer_p0.train(trajectories_p0_by_env, last_states)\n",
    "            trainer_p1.train(trajectories_p1_by_env, last_states)\n",
    "            \n",
    "            fps = frames_in_batch / (time.time() - start_time)\n",
    "            start_time = time.time()\n",
    "\n",
    "            log_data = {\n",
    "                \"charts/total_frames\": total_frames,\n",
    "                \"charts/fps\": fps,\n",
    "                \"charts/learning_rate\": optimizer_p0.param_groups[0][\"lr\"],\n",
    "                \"charts/update_step\": update, # Log the update step\n",
    "            }\n",
    "            \n",
    "            if len(ep_len_buffer) > 0:\n",
    "                log_data[\"episodes/mean_reward_p0\"] = np.mean(ep_info_buffer_p0)\n",
    "                log_data[\"episodes/mean_reward_p1\"] = np.mean(ep_info_buffer_p1)\n",
    "                log_data[\"episodes/mean_length\"] = np.mean(ep_len_buffer)\n",
    "\n",
    "            wandb.log(log_data, commit=True)\n",
    "\n",
    "            ### MODIFIED: Use the new checkpoint manager to save everything ###\n",
    "            if total_frames - last_save_frame >= config.SAVE_EVERY_FRAMES:\n",
    "                checkpoint_manager.save_checkpoint(\n",
    "                    net_p0, net_p1, optimizer_p0, optimizer_p1,\n",
    "                    total_frames, update, wandb_run_id\n",
    "                )\n",
    "                last_save_frame = total_frames\n",
    "\n",
    "        print(\"Training finished.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user.\")\n",
    "    finally:\n",
    "        print(\"Saving final checkpoint and closing all environments...\")\n",
    "        ### MODIFIED: Save final state on exit ###\n",
    "        checkpoint_manager.save_checkpoint(\n",
    "            net_p0, net_p1, optimizer_p0, optimizer_p1,\n",
    "            total_frames, update, wandb_run_id\n",
    "        )\n",
    "        envs.close()\n",
    "        wandb.finish()\n",
    "        print(f\"Cleanup complete. Final checkpoint saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpickleball",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
