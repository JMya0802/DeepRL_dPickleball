{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71624323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import time\n",
    "import typing as tt\n",
    "from dataclasses import dataclass\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.envs.unity_parallel_env import UnityParallelEnv\n",
    "from mlagents_envs.envs.custom_side_channel import CustomDataChannel, StringSideChannel\n",
    "import wandb\n",
    "\n",
    "# --- Config & Hyperparameters ---\n",
    "config_dict = {\n",
    "    # --- Run Identification ---\n",
    "    \"RUN_NAME\": \"d4x21vnd_final_optimized\", \n",
    "\n",
    "    # --- PPO & GAE ---\n",
    "    \"GAMMA\": 0.99,\n",
    "    \"GAE_LAMBDA\": 0.95,\n",
    "    \"PPO_EPSILON\": 0.2,\n",
    "    \"PPO_EPOCHS\": 6,\n",
    "    \"PPO_BATCH_SIZE\": 1024,\n",
    "    \n",
    "    \"VALUE_LOSS_COEF\": 0.5,\n",
    "    \n",
    "    \"ENTROPY_ANNEAL\": True,\n",
    "    \"ENTROPY_COEF_START\": 0.02,   \n",
    "    \"ENTROPY_COEF_END\": 0.005,    \n",
    "\n",
    "    \"LEARNING_RATE\": 1e-4,\n",
    "    \"MIN_LEARNING_RATE\": 1e-5,   \n",
    "    \"LR_SCHEDULE_ANNEAL\": True,\n",
    "    \n",
    "    \"TOTAL_TIMESTEPS\": 50_000_000, \n",
    "\n",
    "    # --- Environment & Sampling ---\n",
    "    \"SKIP_N\": 4,  \n",
    "    \"STACK_N\": 4,\n",
    "    \"MAX_EPISODE_STEPS\": 2000,\n",
    "    \n",
    "    \"NUM_ENVS\": 16, \n",
    "    \"PPO_STEPS_PER_ENV\": 1024,\n",
    "    \"BASE_PORT\": 5004,\n",
    "    \"PORT_STRIDE\": 10,\n",
    "    \"EXECUTABLE_PATH\": r\"C:\\Users\\tan04\\Downloads\\BuildTraining\\BuildTraining\\dp.exe\",\n",
    "\n",
    "    # --- Model Management & Evaluation ---\n",
    "    \"SAVE_EVERY_FRAMES\": 100_000,\n",
    "    \"MODEL_SAVE_PATH\": \"saved_models_ppo_dual_resnet_final\",\n",
    "\n",
    "    # --- Gating / Balancing Mechanism (Hysteresis) ---\n",
    "    \"SCORE_GATING_THRESHOLD\": 2.0, \n",
    "    \"SCORE_RESUME_THRESHOLD\": -1, \n",
    "    \"GATING_WARMUP_EPISODES\": 100, \n",
    "}\n",
    "\n",
    "\n",
    "class CheckpointManager:\n",
    "    \"\"\"Handles saving and loading of the entire training state for resuming.\"\"\"\n",
    "    def __init__(self, base_model_path: str, run_name: str):\n",
    "        self.checkpoint_dir = os.path.join(base_model_path, run_name)\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, \"checkpoint.pth\")\n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "            print(f\"Created checkpoint directory: {self.checkpoint_dir}\")\n",
    "\n",
    "    def save_checkpoint(self, net_p0: nn.Module, net_p1: nn.Module, \n",
    "                        optimizer_p0: optim.Optimizer, optimizer_p1: optim.Optimizer,\n",
    "                        total_frames: int, update: int, wandb_run_id: str,\n",
    "                        p0_active: bool, p1_active: bool):\n",
    "        \"\"\"Saves the complete state needed to resume training, including gating status.\"\"\"\n",
    "        state = {\n",
    "            'net_p0_state_dict': net_p0.state_dict(),\n",
    "            'net_p1_state_dict': net_p1.state_dict(),\n",
    "            'optimizer_p0_state_dict': optimizer_p0.state_dict(),\n",
    "            'optimizer_p1_state_dict': optimizer_p1.state_dict(),\n",
    "            'total_frames': total_frames,\n",
    "            'update': update,\n",
    "            'wandb_run_id': wandb_run_id,\n",
    "            'p0_active': p0_active, \n",
    "            'p1_active': p1_active \n",
    "        }\n",
    "        print(f\"Saving checkpoint to {self.checkpoint_file} at frame {total_frames}...\")\n",
    "        torch.save(state, self.checkpoint_file)\n",
    "        print(\"Checkpoint saved.\")\n",
    "\n",
    "    def load_checkpoint(self, device: torch.device) -> tt.Optional[dict]:\n",
    "        \"\"\"Loads a checkpoint if it exists.\"\"\"\n",
    "        if os.path.exists(self.checkpoint_file):\n",
    "            print(f\"Loading checkpoint from {self.checkpoint_file}...\")\n",
    "            checkpoint = torch.load(self.checkpoint_file, map_location=device)\n",
    "            print(\"Checkpoint loaded.\")\n",
    "            return checkpoint\n",
    "        else:\n",
    "            print(\"No checkpoint found. Starting a new training run.\")\n",
    "            return None\n",
    "\n",
    "# --- Neural Network Classes ---\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        residual = x\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.conv2(out)\n",
    "        out += residual\n",
    "        return self.relu(out)\n",
    "\n",
    "class ActorCriticResNet(nn.Module):\n",
    "    def __init__(self, input_shape: tt.Tuple[int, int, int], n_actions_per_head: tt.List[int]):\n",
    "        super().__init__()\n",
    "        c, h, w = input_shape\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(c, 32, kernel_size=8, stride=4), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1), nn.ReLU(),\n",
    "            ResidualBlock(64),\n",
    "            ResidualBlock(64),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            conv_out_size = self.conv(torch.zeros(1, *input_shape)).size(-1)\n",
    "            \n",
    "        self.fc_base = nn.Sequential(nn.Linear(conv_out_size, 512), nn.ReLU())\n",
    "        \n",
    "        self.action_head_0 = nn.Linear(512, n_actions_per_head[0])\n",
    "        self.action_head_1 = nn.Linear(512, n_actions_per_head[1])\n",
    "        self.action_head_2 = nn.Linear(512, n_actions_per_head[2])\n",
    "        self.value_head = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tt.Tuple[tt.Tuple[torch.Tensor, ...], torch.Tensor]:\n",
    "        base_features = self.fc_base(self.conv(x.float() / 255.0))\n",
    "        logits = (\n",
    "            self.action_head_0(base_features), \n",
    "            self.action_head_1(base_features), \n",
    "            self.action_head_2(base_features)\n",
    "        )\n",
    "        value = self.value_head(base_features)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "# --- Environment Wrappers ---\n",
    "class UnityEnvWrapper:\n",
    "    def __init__(self, env: UnityParallelEnv, max_episode_steps: int):\n",
    "        self.env = env\n",
    "        self.max_episode_steps = max_episode_steps\n",
    "        self.agent_ids = self.env.agents\n",
    "        self.p0_id, self.p1_id = self.agent_ids[0], self.agent_ids[1]\n",
    "        self._step_count = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def _preprocess_obs(img: np.ndarray) -> np.ndarray:\n",
    "        # Grayscale and Crop\n",
    "        gray = 0.299 * img[0] + 0.587 * img[1] + 0.114 * img[2]\n",
    "        return (gray[np.newaxis, 24:-6, 18:-18] * 255).astype(np.uint8)\n",
    "\n",
    "    def reset(self) -> np.ndarray:\n",
    "        self._step_count = 0\n",
    "        observations = self.env.reset()\n",
    "        img = observations[self.p0_id]['observation'][0]\n",
    "        return self._preprocess_obs(img)\n",
    "\n",
    "    def step(self, actions: dict) -> tt.Tuple[np.ndarray, dict, bool, dict]:\n",
    "        observations, rewards, dones, infos = self.env.step(actions)\n",
    "        img = observations[self.p0_id]['observation'][0]\n",
    "        processed_frame = self._preprocess_obs(img)\n",
    "        step_rewards = {self.p0_id: float(rewards.get(self.p0_id, 0.0)), self.p1_id: float(rewards.get(self.p1_id, 0.0))}\n",
    "        done_env = bool(dones.get('__all__', False))\n",
    "        self._step_count += 1\n",
    "        truncated = self._step_count >= self.max_episode_steps\n",
    "        done = done_env or truncated\n",
    "        return processed_frame, step_rewards, done, {}\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "class Multi_Env_With_SkipN_and_Stack:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.envs: tt.List[UnityEnvWrapper] = []\n",
    "        self._frame_stacks: tt.List[collections.deque] = []\n",
    "        \n",
    "        print(f\"Starting {self.config.NUM_ENVS} environments...\")\n",
    "        for i in range(self.config.NUM_ENVS):\n",
    "            port = self.config.BASE_PORT + i * self.config.PORT_STRIDE\n",
    "            channel = CustomDataChannel()\n",
    "            string_channel = StringSideChannel()\n",
    "            channel.send_data(serve=212, p1=0, p2=0) \n",
    "            unity_env = UnityEnvironment(file_name=self.config.EXECUTABLE_PATH, base_port=port, side_channels=[string_channel, channel])\n",
    "            wrapped_env = UnityEnvWrapper(UnityParallelEnv(unity_env), self.config.MAX_EPISODE_STEPS)\n",
    "            self.envs.append(wrapped_env)\n",
    "            self._frame_stacks.append(collections.deque(maxlen=self.config.STACK_N))\n",
    "            print(f\"  - Env {i} started on port {port}.\")\n",
    "\n",
    "        first_env = self.envs[0]\n",
    "        self.p0_id, self.p1_id = first_env.p0_id, first_env.p1_id\n",
    "        \n",
    "        self.states = self.reset()\n",
    "        \n",
    "    def _get_env_state(self, frame_stack: collections.deque) -> np.ndarray:\n",
    "        return np.concatenate(list(frame_stack), axis=0)\n",
    "\n",
    "    def reset(self) -> tt.List[np.ndarray]:\n",
    "        for i, env in enumerate(self.envs):\n",
    "            first_frame = env.reset()\n",
    "            for _ in range(self.config.STACK_N):\n",
    "                self._frame_stacks[i].append(first_frame)\n",
    "        return [self._get_env_state(fs) for fs in self._frame_stacks]\n",
    "\n",
    "    @staticmethod\n",
    "    def _decode_action(action_index: int) -> np.ndarray:\n",
    "        a0 = action_index % 3\n",
    "        a1 = (action_index // 3) % 3\n",
    "        a2 = (action_index // 9) % 3\n",
    "        return np.array([a0, a1, a2], dtype=np.int32)\n",
    "\n",
    "    def step(self, action_idxs_p0: tt.List[int], action_idxs_p1: tt.List[int]) -> tt.Tuple[tt.List[np.ndarray], dict, tt.List[bool]]:\n",
    "        num_envs = len(self.envs)\n",
    "        cum_p0_rewards = [0.0] * num_envs\n",
    "        cum_p1_rewards = [0.0] * num_envs\n",
    "        dones = [False] * num_envs\n",
    "        last_frames = [None] * num_envs\n",
    "\n",
    "        decoded_p0_actions = [self._decode_action(idx) for idx in action_idxs_p0]\n",
    "        decoded_p1_actions = [self._decode_action(idx) for idx in action_idxs_p1]\n",
    "\n",
    "        for _ in range(self.config.SKIP_N):\n",
    "            for i, env in enumerate(self.envs):\n",
    "                if not dones[i]:\n",
    "                    env_actions = {self.p0_id: decoded_p0_actions[i], self.p1_id: decoded_p1_actions[i]}\n",
    "                    new_frame, rewards, done, _ = env.step(env_actions)\n",
    "                    \n",
    "                    cum_p0_rewards[i] += rewards[self.p0_id] - rewards[self.p1_id]\n",
    "                    cum_p1_rewards[i] += rewards[self.p1_id] - rewards[self.p0_id]\n",
    "                    \n",
    "                    last_frames[i] = new_frame\n",
    "                    if done:\n",
    "                        dones[i] = True\n",
    "        \n",
    "        for i in range(num_envs):\n",
    "            if dones[i]:\n",
    "                first_frame = self.envs[i].reset()\n",
    "                self._frame_stacks[i].clear()\n",
    "                for _ in range(self.config.STACK_N):\n",
    "                    self._frame_stacks[i].append(first_frame)\n",
    "            else:\n",
    "                self._frame_stacks[i].append(last_frames[i])\n",
    "        \n",
    "        new_states = [self._get_env_state(fs) for fs in self._frame_stacks]\n",
    "        reward_dict = {self.p0_id: cum_p0_rewards, self.p1_id: cum_p1_rewards}\n",
    "        \n",
    "        return new_states, reward_dict, dones\n",
    "\n",
    "    def close(self):\n",
    "        for env in self.envs: env.close()\n",
    "\n",
    "# --- PPO Implementation ---\n",
    "@dataclass\n",
    "class TrajectoryItem:\n",
    "    state: np.ndarray\n",
    "    action: int\n",
    "    reward: float\n",
    "    done: bool\n",
    "    log_prob: float\n",
    "    value: float\n",
    "\n",
    "class Collector:\n",
    "    def __init__(self, envs: Multi_Env_With_SkipN_and_Stack, config):\n",
    "        self.envs = envs\n",
    "        self.config = config\n",
    "        self.current_states = self.envs.states\n",
    "\n",
    "    def collect_trajectories(self, net_p0: ActorCriticResNet, net_p1: ActorCriticResNet, device: torch.device) -> tt.Tuple[dict, dict, np.ndarray]:\n",
    "        trajectories_p0 = {i: [] for i in range(self.config.NUM_ENVS)}\n",
    "        trajectories_p1 = {i: [] for i in range(self.config.NUM_ENVS)}\n",
    "        \n",
    "        for _ in range(self.config.PPO_STEPS_PER_ENV):\n",
    "            states_v = torch.from_numpy(np.array(self.current_states)).to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits_p0, values_p0_v = net_p0(states_v)\n",
    "                dists_p0 = [Categorical(logits=l) for l in logits_p0]\n",
    "                actions_p0 = [d.sample() for d in dists_p0]\n",
    "                log_probs_p0 = sum(d.log_prob(a) for d, a in zip(dists_p0, actions_p0))\n",
    "                action_idxs_p0 = actions_p0[0] + 3 * actions_p0[1] + 9 * actions_p0[2]\n",
    "\n",
    "                logits_p1, values_p1_v = net_p1(states_v)\n",
    "                dists_p1 = [Categorical(logits=l) for l in logits_p1]\n",
    "                actions_p1 = [d.sample() for d in dists_p1]\n",
    "                log_probs_p1 = sum(d.log_prob(a) for d, a in zip(dists_p1, actions_p1))\n",
    "                action_idxs_p1 = actions_p1[0] + 3 * actions_p1[1] + 9 * actions_p1[2]\n",
    "\n",
    "            new_states, rewards_dict, dones = self.envs.step(\n",
    "                action_idxs_p0.cpu().numpy(), \n",
    "                action_idxs_p1.cpu().numpy()\n",
    "            )\n",
    "            \n",
    "            for i in range(self.config.NUM_ENVS):\n",
    "                trajectories_p0[i].append(TrajectoryItem(\n",
    "                    state=self.current_states[i], action=action_idxs_p0[i].item(),\n",
    "                    reward=rewards_dict[self.envs.p0_id][i], done=dones[i],\n",
    "                    log_prob=log_probs_p0[i].item(), value=values_p0_v[i].item()\n",
    "                ))\n",
    "                trajectories_p1[i].append(TrajectoryItem(\n",
    "                    state=self.current_states[i], action=action_idxs_p1[i].item(),\n",
    "                    reward=rewards_dict[self.envs.p1_id][i], done=dones[i],\n",
    "                    log_prob=log_probs_p1[i].item(), value=values_p1_v[i].item()\n",
    "                ))\n",
    "            \n",
    "            self.current_states = new_states\n",
    "        \n",
    "        last_states = np.array(self.current_states)\n",
    "        return trajectories_p0, trajectories_p1, last_states\n",
    "\n",
    "class PPOTrainer:\n",
    "    def __init__(self, net: ActorCriticResNet, optimizer: optim.Optimizer, device: torch.device, agent_id: str, config):\n",
    "        self.net = net\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.agent_id = agent_id\n",
    "        self.config = config\n",
    "\n",
    "    def _calculate_advantages_and_returns(self, trajectories_by_env: tt.Dict[int, tt.List[TrajectoryItem]], last_states: np.ndarray):\n",
    "        all_advantages, all_returns = [], []\n",
    "        last_states_v = torch.from_numpy(last_states).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            _, last_vals_v = self.net(last_states_v)\n",
    "        \n",
    "        for i in range(len(trajectories_by_env)):\n",
    "            advantages, returns = [], []\n",
    "            gae_advantage = 0.0\n",
    "            next_value = last_vals_v[i].item()\n",
    "            if trajectories_by_env[i][-1].done:\n",
    "                next_value = 0.0\n",
    "            \n",
    "            for traj in reversed(trajectories_by_env[i]):          \n",
    "                delta = traj.reward + self.config.GAMMA * next_value * (1 - int(traj.done)) - traj.value\n",
    "                gae_advantage = delta + self.config.GAMMA * self.config.GAE_LAMBDA * gae_advantage * (1 - int(traj.done))\n",
    "                returns.append(gae_advantage + traj.value)\n",
    "                advantages.append(gae_advantage)\n",
    "                next_value = traj.value\n",
    "            \n",
    "            all_advantages.extend(list(reversed(advantages)))\n",
    "            all_returns.extend(list(reversed(returns)))\n",
    "\n",
    "        return all_advantages, all_returns\n",
    "\n",
    "    def train(self, trajectories_by_env: tt.Dict[int, tt.List[TrajectoryItem]], last_states: np.ndarray, current_entropy_coef: float):\n",
    "        trajectories = [item for sublist in trajectories_by_env.values() for item in sublist]\n",
    "        advantages, returns = self._calculate_advantages_and_returns(trajectories_by_env, last_states)\n",
    "        \n",
    "        states = np.array([t.state for t in trajectories])\n",
    "        actions = np.array([t.action for t in trajectories])\n",
    "        log_probs_old = np.array([t.log_prob for t in trajectories], dtype=np.float32)\n",
    "        advantages = np.array(advantages, dtype=np.float32)\n",
    "        returns = np.array(returns, dtype=np.float32)\n",
    "        values_old = np.array([t.value for t in trajectories], dtype=np.float32)\n",
    "\n",
    "        var_y = np.var(returns)\n",
    "        explained_var = 1 - np.var(returns - values_old) / (var_y + 1e-8) if var_y > 1e-8 else 0\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        for _ in range(self.config.PPO_EPOCHS):\n",
    "            indices = np.random.permutation(len(trajectories))\n",
    "            for start in range(0, len(trajectories), self.config.PPO_BATCH_SIZE):\n",
    "                end = start + self.config.PPO_BATCH_SIZE\n",
    "                batch_indices = indices[start:end]                  \n",
    "                \n",
    "                batch_states = torch.from_numpy(states[batch_indices]).to(self.device)\n",
    "                batch_actions_v = torch.from_numpy(actions[batch_indices]).to(self.device)\n",
    "                batch_log_probs_old_v = torch.from_numpy(log_probs_old[batch_indices]).to(self.device)\n",
    "                batch_advantages_v = torch.from_numpy(advantages[batch_indices]).to(self.device)\n",
    "                batch_returns_v = torch.from_numpy(returns[batch_indices]).to(self.device)\n",
    "                \n",
    "                logits, values_v = self.net(batch_states)\n",
    "                values_v = values_v.squeeze(-1)\n",
    "\n",
    "                dists = [Categorical(logits=l) for l in logits]\n",
    "                a0 = batch_actions_v % 3\n",
    "                a1 = (batch_actions_v // 3) % 3\n",
    "                a2 = (batch_actions_v // 9) % 3\n",
    "                log_probs_new = dists[0].log_prob(a0) + dists[1].log_prob(a1) + dists[2].log_prob(a2)\n",
    "                \n",
    "                ratio = torch.exp(log_probs_new - batch_log_probs_old_v)\n",
    "                surr1 = ratio * batch_advantages_v\n",
    "                surr2 = torch.clamp(ratio, 1.0 - self.config.PPO_EPSILON, 1.0 + self.config.PPO_EPSILON) * batch_advantages_v\n",
    "                policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "                value_loss = nn.functional.mse_loss(values_v, batch_returns_v)\n",
    "                entropy = sum(d.entropy().mean() for d in dists)\n",
    "                \n",
    "                total_loss = (policy_loss + self.config.VALUE_LOSS_COEF * value_loss - current_entropy_coef * entropy)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.net.parameters(), max_norm=0.5)\n",
    "                self.optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            log_ratio = log_probs_new - batch_log_probs_old_v\n",
    "            approx_kl = torch.mean((torch.exp(log_ratio) - 1) - log_ratio).item()\n",
    "            clip_fraction = torch.mean((torch.abs(ratio - 1.0) > self.config.PPO_EPSILON).float()).item()\n",
    "        \n",
    "        wandb.log({\n",
    "            f\"train/{self.agent_id}_policy_loss\": policy_loss.item(),\n",
    "            f\"train/{self.agent_id}_value_loss\": value_loss.item(),\n",
    "            f\"train/{self.agent_id}_entropy\": entropy.item(),\n",
    "            f\"train/{self.agent_id}_explained_variance\": explained_var,\n",
    "            f\"train/{self.agent_id}_approx_kl\": approx_kl,\n",
    "            f\"train/{self.agent_id}_clip_fraction\": clip_fraction,\n",
    "        }, commit=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 1. Initialization\n",
    "    RUN_NAME = config_dict.get(\"RUN_NAME\", wandb.util.generate_id())\n",
    "    MODEL_SAVE_PATH = config_dict.get(\"MODEL_SAVE_PATH\", \"saved_models_ppo_dual_resnet_final\")\n",
    "\n",
    "    checkpoint_manager = CheckpointManager(MODEL_SAVE_PATH, RUN_NAME)\n",
    "    checkpoint = checkpoint_manager.load_checkpoint(device)\n",
    "\n",
    "    # 3. State variables\n",
    "    total_frames = 0\n",
    "    start_update = 1\n",
    "    last_save_frame = 0\n",
    "    wandb_run_id = None \n",
    "    update_for_save = 1 \n",
    "    \n",
    "    # Gating States (Persist across updates)\n",
    "    p0_training_active = True\n",
    "    p1_training_active = True\n",
    "\n",
    "    # 4. Resume Logic\n",
    "    if checkpoint:\n",
    "        try:\n",
    "            wandb_run_id = checkpoint['wandb_run_id']\n",
    "            total_frames = checkpoint['total_frames']\n",
    "            start_update = checkpoint['update'] + 1\n",
    "            update_for_save = start_update \n",
    "            last_save_frame = total_frames\n",
    "            \n",
    "            # Load gating states if available, else default to True\n",
    "            p0_training_active = checkpoint.get('p0_active', True)\n",
    "            p1_training_active = checkpoint.get('p1_active', True)\n",
    "            \n",
    "            print(f\"Found checkpoint for W&B run ID: {wandb_run_id}. Resuming from frame {total_frames}.\")\n",
    "            print(f\"Restored gating states: P0 Active={p0_training_active}, P1 Active={p1_training_active}\")\n",
    "        except KeyError as e:\n",
    "            print(f\"Checkpoint file is corrupt or missing key: {e}. Starting a new run.\")\n",
    "            checkpoint = None \n",
    "            wandb_run_id = None\n",
    "    \n",
    "    # 5. Initialize W&B\n",
    "    if wandb_run_id:\n",
    "        run = wandb.init(\n",
    "            project=\"PPO-Skip-N-Step-Parallel-Resumable\",\n",
    "            config=config_dict,\n",
    "            id=wandb_run_id,\n",
    "            name=RUN_NAME, \n",
    "            resume=\"must\"\n",
    "        )\n",
    "    else:\n",
    "        print(\"Starting a new training run.\")\n",
    "        run = wandb.init(\n",
    "            project=\"PPO-Skip-N-Step-Parallel-Resumable\",\n",
    "            config=config_dict,\n",
    "            name=RUN_NAME, \n",
    "        )\n",
    "    \n",
    "    wandb_run_id = run.id\n",
    "    config = run.config \n",
    "    print(f\"W&B Run ID: {wandb_run_id}\")\n",
    "\n",
    "    # 7. Initialize envs, models, optimizers\n",
    "    envs = Multi_Env_With_SkipN_and_Stack(config)\n",
    "    actual_input_shape = envs.states[0].shape\n",
    "    print(f\"Detected observation shape from environment: {actual_input_shape}\")\n",
    "    \n",
    "    n_actions_per_head = [3, 3, 3]\n",
    "\n",
    "    net_p0 = ActorCriticResNet(actual_input_shape, n_actions_per_head).to(device)\n",
    "    net_p1 = ActorCriticResNet(actual_input_shape, n_actions_per_head).to(device)\n",
    "    \n",
    "    optimizer_p0 = optim.Adam(net_p0.parameters(), lr=config.LEARNING_RATE, eps=1e-5)\n",
    "    optimizer_p1 = optim.Adam(net_p1.parameters(), lr=config.LEARNING_RATE, eps=1e-5)\n",
    "\n",
    "    # 8. Load model/optimizer states\n",
    "    if checkpoint:\n",
    "        print(\"Loading model and optimizer states from checkpoint...\")\n",
    "        net_p0.load_state_dict(checkpoint['net_p0_state_dict'])\n",
    "        net_p1.load_state_dict(checkpoint['net_p1_state_dict'])\n",
    "        optimizer_p0.load_state_dict(checkpoint['optimizer_p0_state_dict'])\n",
    "        optimizer_p1.load_state_dict(checkpoint['optimizer_p1_state_dict'])\n",
    "        print(\"States loaded successfully.\")\n",
    "    \n",
    "    collector = Collector(envs, config)\n",
    "    trainer_p0 = PPOTrainer(net_p0, optimizer_p0, device, agent_id=\"p0\", config=config)\n",
    "    trainer_p1 = PPOTrainer(net_p1, optimizer_p1, device, agent_id=\"p1\", config=config)\n",
    "    \n",
    "    ep_info_buffer_p0 = collections.deque(maxlen=100)\n",
    "    ep_info_buffer_p1 = collections.deque(maxlen=100)\n",
    "    ep_len_buffer = collections.deque(maxlen=100)\n",
    "    \n",
    "    current_ep_rewards_p0 = np.zeros(config.NUM_ENVS, dtype=np.float32)\n",
    "    current_ep_rewards_p1 = np.zeros(config.NUM_ENVS, dtype=np.float32)\n",
    "    current_ep_lengths = np.zeros(config.NUM_ENVS, dtype=np.int32)\n",
    "    \n",
    "    # --- Main Training Loop ---\n",
    "    start_time = time.time()\n",
    "    num_updates = config.TOTAL_TIMESTEPS // (config.NUM_ENVS * config.PPO_STEPS_PER_ENV)\n",
    "    \n",
    "    try:\n",
    "        for update in tqdm(range(start_update, num_updates + 1), desc=\"PPO Updates\", initial=start_update, total=num_updates):\n",
    "            update_for_save = update \n",
    "            \n",
    "            # --- Schedule: Learning Rate & Entropy ---\n",
    "            frac = 1.0 - (update - 1.0) / num_updates\n",
    "\n",
    "            if config.LR_SCHEDULE_ANNEAL:\n",
    "                min_lr = config.MIN_LEARNING_RATE\n",
    "                max_lr = config.LEARNING_RATE\n",
    "                new_lr = min_lr + (max_lr - min_lr) * frac\n",
    "                optimizer_p0.param_groups[0][\"lr\"] = new_lr\n",
    "                optimizer_p1.param_groups[0][\"lr\"] = new_lr\n",
    "\n",
    "            if config_dict.get(\"ENTROPY_ANNEAL\", False):\n",
    "                start_e = config_dict[\"ENTROPY_COEF_START\"]\n",
    "                end_e = config_dict[\"ENTROPY_COEF_END\"]\n",
    "                current_entropy_coef = end_e + (start_e - end_e) * frac\n",
    "            else:\n",
    "                current_entropy_coef = 0.01 # Default fallback\n",
    "\n",
    "            # -----------------------------------------\n",
    "\n",
    "            trajectories_p0_by_env, trajectories_p1_by_env, last_states = collector.collect_trajectories(net_p0, net_p1, device)\n",
    "            \n",
    "            frames_in_batch = sum(len(traj_list) for traj_list in trajectories_p0_by_env.values()) * config.SKIP_N\n",
    "            total_frames += frames_in_batch\n",
    "            \n",
    "            for i in range(config.NUM_ENVS):\n",
    "                for t_item_p0, t_item_p1 in zip(trajectories_p0_by_env[i], trajectories_p1_by_env[i]):\n",
    "                    current_ep_rewards_p0[i] += t_item_p0.reward\n",
    "                    current_ep_rewards_p1[i] += t_item_p1.reward\n",
    "                    current_ep_lengths[i] += 1\n",
    "                    \n",
    "                    if t_item_p0.done:\n",
    "                        ep_info_buffer_p0.append(current_ep_rewards_p0[i])\n",
    "                        ep_info_buffer_p1.append(current_ep_rewards_p1[i])\n",
    "                        ep_len_buffer.append(current_ep_lengths[i])\n",
    "                        \n",
    "                        current_ep_rewards_p0[i] = 0\n",
    "                        current_ep_rewards_p1[i] = 0\n",
    "                        current_ep_lengths[i] = 0\n",
    "\n",
    "            # --- Gating Mechanism Start (Hysteresis Logic) ---\n",
    "            PAUSE_THRESHOLD = config.SCORE_GATING_THRESHOLD\n",
    "            RESUME_THRESHOLD = config.SCORE_RESUME_THRESHOLD\n",
    "\n",
    "            mean_score_p0 = 0\n",
    "            mean_score_p1 = 0\n",
    "            score_diff = 0.0\n",
    "\n",
    "            if len(ep_info_buffer_p0) >= config.GATING_WARMUP_EPISODES:\n",
    "                mean_score_p0 = np.mean(ep_info_buffer_p0)\n",
    "                mean_score_p1 = np.mean(ep_info_buffer_p1)\n",
    "                score_diff = mean_score_p0 - mean_score_p1\n",
    "                \n",
    "                # --- P0 Control Logic ---\n",
    "                if p0_training_active:\n",
    "                    if score_diff > PAUSE_THRESHOLD:\n",
    "                        p0_training_active = False\n",
    "                else:\n",
    "                    if score_diff < RESUME_THRESHOLD:\n",
    "                        p0_training_active = True\n",
    "\n",
    "                # --- P1 Control Logic (Symmetric) ---\n",
    "                if p1_training_active:\n",
    "                    if score_diff < -PAUSE_THRESHOLD:\n",
    "                        p1_training_active = False\n",
    "                else:\n",
    "                    if score_diff > -RESUME_THRESHOLD:\n",
    "                        p1_training_active = True\n",
    "            \n",
    "            should_train_p0 = p0_training_active\n",
    "            should_train_p1 = p1_training_active\n",
    "            # --- Gating Mechanism End ---\n",
    "\n",
    "            if should_train_p0:\n",
    "                trainer_p0.train(trajectories_p0_by_env, last_states, current_entropy_coef)\n",
    "            \n",
    "            if should_train_p1:\n",
    "                trainer_p1.train(trajectories_p1_by_env, last_states, current_entropy_coef)\n",
    "            \n",
    "            fps = frames_in_batch / (time.time() - start_time)\n",
    "            start_time = time.time()\n",
    "\n",
    "            log_data = {\n",
    "                \"charts/total_frames\": total_frames,\n",
    "                \"charts/fps\": fps,\n",
    "                \"charts/learning_rate\": optimizer_p0.param_groups[0][\"lr\"],\n",
    "                \"charts/entropy_coef\": current_entropy_coef,\n",
    "                \"charts/update_step\": update, \n",
    "                \"gating/score_diff\": score_diff,\n",
    "                \"gating/train_p0_active\": int(should_train_p0), \n",
    "                \"gating/train_p1_active\": int(should_train_p1), \n",
    "            }\n",
    "            \n",
    "            if len(ep_len_buffer) > 0:\n",
    "                log_data[\"episodes/mean_reward_p0\"] = np.mean(ep_info_buffer_p0)\n",
    "                log_data[\"episodes/mean_reward_p1\"] = np.mean(ep_info_buffer_p1)\n",
    "                log_data[\"episodes/mean_length\"] = np.mean(ep_len_buffer)\n",
    "\n",
    "            wandb.log(log_data, commit=True)\n",
    "\n",
    "            if total_frames - last_save_frame >= config.SAVE_EVERY_FRAMES:\n",
    "                checkpoint_manager.save_checkpoint(\n",
    "                    net_p0, net_p1, optimizer_p0, optimizer_p1,\n",
    "                    total_frames, update, wandb_run_id,\n",
    "                    p0_training_active, p1_training_active\n",
    "                )\n",
    "                last_save_frame = total_frames\n",
    "\n",
    "        print(\"Training finished.\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user.\")\n",
    "    finally:\n",
    "        print(\"Saving final checkpoint and closing all environments...\")\n",
    "        checkpoint_manager.save_checkpoint(\n",
    "            net_p0, net_p1, optimizer_p0, optimizer_p1,\n",
    "            total_frames, update_for_save, wandb_run_id,\n",
    "            p0_training_active, p1_training_active\n",
    "        )\n",
    "        envs.close()\n",
    "        wandb.finish()\n",
    "        print(f\"Cleanup complete. Final checkpoint saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpickleball",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
